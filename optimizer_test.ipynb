{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0873cc70-cffc-4bb6-ab54-83a827fb1000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tinytorch.core.tensor import Tensor\n",
    "from tinytorch.core.autograd import enable_autograd\n",
    "from tinytorch.core.optimizers import Optimizer, SGD, Adam, AdamW\n",
    "\n",
    "enable_autograd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f125da-9e1b-4642-93e7-c2e821509f23",
   "metadata": {},
   "source": [
    "## Unit Test -Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f78940c2-104e-4e72-9ba9-09f0692f06eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Unit Test: Base Optimizer...\n",
      "âœ… Base Optimizer works correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_optimizer_base():\n",
    "    print(\"ðŸ”¬ Unit Test: Base Optimizer...\")\n",
    "\n",
    "    # create test parameters\n",
    "    param1 = Tensor([1.0, 2.0], requires_grad= True)\n",
    "    param2 = Tensor([[3.0, 4.0], [5.0, 6.0]], requires_grad= True)\n",
    "\n",
    "    # add some gradients\n",
    "    param1.grad = Tensor([0.1, 0.2])\n",
    "    param2.grad = Tensor([[0.3, 0.4], [0.5, 0.6]])\n",
    "\n",
    "    # create an optimizer\n",
    "    optimizer = Optimizer([param1, param2])\n",
    "\n",
    "    # Test parameter storage\n",
    "    assert len(optimizer.params) == 2\n",
    "    assert optimizer.params[0] is param1\n",
    "    assert optimizer.params[1] is param2\n",
    "    assert optimizer.step_count == 0\n",
    "\n",
    "    # Test zero_grad\n",
    "    optimizer.zero_grad()\n",
    "    assert param1.grad is None\n",
    "    assert param2.grad is None\n",
    "\n",
    "    # Test that optimizers accepts any tensor (no validation required)\n",
    "    regular_param = Tensor([1.0])\n",
    "    opt = Optimizer([regular_param])\n",
    "    assert len(opt.params) == 1\n",
    "    \n",
    "    \n",
    "\n",
    "    print(\"âœ… Base Optimizer works correctly!\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    test_unit_optimizer_base()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7947072e-333e-4121-9e46-f2032134ae2e",
   "metadata": {},
   "source": [
    "## Unit Test - Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0321d2b2-1db5-46cc-a303-e35facb546da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Unit Test: SGD Optimizer...\n",
      "âœ… SGD optimizer works correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_sgd_optimizer():\n",
    "    print(\"ðŸ”¬ Unit Test: SGD Optimizer...\")\n",
    "\n",
    "    # Test basic SGD without momentum\n",
    "    param = Tensor([1.0, 2.0], requires_grad= True)\n",
    "    param.grad = Tensor([0.1, 0.2])\n",
    "\n",
    "    optimizer = SGD([param], lr=0.1)\n",
    "    original_data = param.data.copy()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    # Expected: param = param - lr * grad = [1.0, 2.0] - 0.1 * [0.1, 0.2] = [0.99, 1.98]\n",
    "    expected = original_data - 0.1 * param.grad.data\n",
    "    assert np.allclose(param.data, expected)\n",
    "    assert optimizer.step_count == 1\n",
    "\n",
    "    # Test SGD with momentum\n",
    "    param2 = Tensor([1.0, 2.0], requires_grad = True)\n",
    "    param2.grad = Tensor([0.1, 0.2])\n",
    "\n",
    "    optimizer_momentum = SGD([param2], lr=0.1, momentum=0.9)\n",
    "    # first step v = 0.9 * 0 + [0.1, 0.2] = [0.1, 0.2]\n",
    "    optimizer_momentum.step()\n",
    "    expected_first = np.array([1.0, 2.0] - 0.1 * np.array([0.1, 0.2]))\n",
    "    np.allclose(param2.data, expected_first)\n",
    "\n",
    "    # second setp with the same gradient\n",
    "    param2.grad = Tensor([0.1, 0.2])\n",
    "    optimizer_momentum.step()\n",
    "    # v = 0.9 * [0.1, 0.2] + [0.1, 0.2] = [0.19, 0.38]\n",
    "    expected_momentum = np.array([0.19, 0.38])\n",
    "    expected_second = expected_first - 0.1 * expected_momentum\n",
    "    assert np.allclose(param2.data, expected_second, rtol=1e-5)\n",
    "\n",
    "    # Test weigth decay\n",
    "    param3 = Tensor([1.0, 2.0], requires_grad= True)\n",
    "    param3.grad = Tensor([0.1, 0.2], requires_grad= True)\n",
    "\n",
    "    optimizer_wd = SGD([param3], lr= 0.1, weight_decay=0.01)\n",
    "    optimizer_wd.step()\n",
    "\n",
    "    # grad with with decay = [0.1, 0.2] + 0.01 * [1.0, 2.0] = [0.11, 0.22] i.e. grad + weight_decay * param.data\n",
    "    expected_wd = np.array([1.0, 2.0]) - 0.1 * np.array([0.11, 0.22])\n",
    "    assert np.allclose(param3.data, expected_wd)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    print(\"âœ… SGD optimizer works correctly!\")\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    test_unit_sgd_optimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc40f38-37b4-4594-8a2e-68b84657af03",
   "metadata": {},
   "source": [
    "## Unit Test Adam Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6436031-c40c-4819-adb8-4f3003c8deaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Unit Test: Adam Optimizer...\n",
      "âœ… Adam optimizer works correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_adam_optimizer():\n",
    "    print(\"ðŸ”¬ Unit Test: Adam Optimizer...\")\n",
    "\n",
    "    # Test basic Adam functionality\n",
    "    param = Tensor([1.0, 2.0], requires_grad= True)\n",
    "    param.grad = Tensor([0.1, 0.2])\n",
    "\n",
    "    optimizer = Adam([param], lr=0.01, betas= (0.9, 0.999), eps=1e-8)\n",
    "    original_data = param.data.copy()\n",
    "\n",
    "    # Frist step\n",
    "    optimizer.step()\n",
    "\n",
    "    # manual compute expected values\n",
    "    grad = np.array([0.1, 0.2])\n",
    "    # first moment\n",
    "    m = 0.1 * grad\n",
    "\n",
    "    # second moment\n",
    "    v = 0.001 * (grad ** 2)\n",
    "\n",
    "    # bias corrections\n",
    "    bias_correction1 = 1 - 0.9 ** 1\n",
    "    bias_correction2 = 1 - 0.999**1\n",
    "\n",
    "    m_hat = m / bias_correction1\n",
    "    v_hat = v / bias_correction2\n",
    "\n",
    "    expected = original_data - 0.01 * m_hat / (np.sqrt(v_hat)+ 1e-8)\n",
    "    assert np.allclose(param.data, expected, rtol=1e-6)\n",
    "    assert optimizer.step_count == 1\n",
    "\n",
    "    # Test second step to verify moment accumulation\n",
    "    param.grad = Tensor([0.1, 0.2])\n",
    "    optimizer.step()\n",
    "\n",
    "    # should have updated moments\n",
    "    assert optimizer.m_buffers[0] is not None\n",
    "    assert optimizer.v_buffers[0] is not None\n",
    "    assert optimizer.step_count == 2\n",
    "\n",
    "    # Test with weight decay\n",
    "    param2  = Tensor([1.0, 2.0], requires_grad= True)\n",
    "    param2.grad = Tensor([0.1, 0.2])\n",
    "\n",
    "    optimizer_wd = Adam([param2], lr= 0.01, weight_decay=0.01)\n",
    "    optimizer_wd.step()\n",
    "\n",
    "    # weight with decay computation \n",
    "    # grad_with_decay = [0.1, 0.2] + 0.01 * [1.0, 2.0] = [0.11, 0.22]\n",
    "    assert not np.array_equal(param2.data, np.array([1.0, 2.0]))\n",
    "\n",
    "    print(\"âœ… Adam optimizer works correctly!\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    test_unit_adam_optimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0990e6e9-cd85-41bc-9a20-5630e1e25632",
   "metadata": {},
   "source": [
    "## Unit Test - AdamW Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bfed64f-6f12-4ef4-bde2-f9840b82a3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Unit Test: AdamW Optimizer...\n",
      "âœ… AdamW optimizer works correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_adamw_optimizer():\n",
    "    print(\"ðŸ”¬ Unit Test: AdamW Optimizer...\")\n",
    "\n",
    "    # Test AdamW vs Adam difference in weight decay\n",
    "    # Create identical parameters for comparison\n",
    "    param_adam = Tensor([1.0, 2.0], requires_grad= True)\n",
    "    param_adamw = Tensor([1.0, 2.0], requires_grad= True)\n",
    "\n",
    "    param_adam.grad = Tensor([0.1, 0.2])\n",
    "    param_adamw.grad = Tensor([0.1, 0.2])\n",
    "\n",
    "    # create optimzers with same settings\n",
    "    adam = Adam([param_adam], lr=0.01, weight_decay= 0.01)\n",
    "    adamw = AdamW([param_adamw], lr= 0.01, weight_decay= 0.01)\n",
    "\n",
    "    # Take one step\n",
    "    adam.step()\n",
    "    adamw.step()\n",
    "\n",
    "    assert not np.allclose(param_adam.data, param_adamw.data, rtol=1e-6)\n",
    "\n",
    "    # Test AdamW basic functionality\n",
    "    param = Tensor([1.0, 2.0], requires_grad= True)\n",
    "    param.grad = Tensor([0.1, 0.2])\n",
    "\n",
    "    optimizer = AdamW([param], lr=0.01, weight_decay= 0.01)\n",
    "    original_data = param.data.copy()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    # parameters should have changed\n",
    "    assert not np.array_equal(param.data, original_data)\n",
    "    assert optimizer.step_count == 1\n",
    "\n",
    "    # Test that moment buffers are created\n",
    "    assert optimizer.m_buffers[0] is not None\n",
    "    assert optimizer.v_buffers[0] is not None\n",
    "\n",
    "    # Test zero weight decay behaves like Adam\n",
    "    param1 = Tensor([1.0, 2.0], requires_grad= True)\n",
    "    param2 = Tensor([1.0, 2.0], requires_grad = True)\n",
    "\n",
    "    param1.grad = Tensor([0.1, 0.2])\n",
    "    param2.grad = Tensor([0.1, 0.2])\n",
    "\n",
    "    adam_no_wd = Adam([param1], lr= 0.01, weight_decay= 0.0)\n",
    "    adamw_no_wd = AdamW([param2], lr= 0.01, weight_decay= 0.0)\n",
    "\n",
    "    adam_no_wd.step()\n",
    "    adamw_no_wd.step()\n",
    "\n",
    "    # should be very simular (within numerical precision)\n",
    "    assert np.allclose(param1.data, param2.data, rtol=1e-10)\n",
    "    \n",
    "\n",
    "    print(\"âœ… AdamW optimizer works correctly!\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    test_unit_adamw_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3fc67f4-432b-4d24-b284-9b5791fefe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_optimizer_memory_usage():\n",
    "    \"\"\"ðŸ“Š Analyze memory usage of different optimizers.\"\"\"\n",
    "    print(\"ðŸ“Š Analyzing Optimizer Memory Usage...\")\n",
    "\n",
    "    # Create test parameters of different sizes\n",
    "    param_sizes = [1000, 10000, 100000]  # 1K, 10K, 100K parameters\n",
    "\n",
    "    print(\"Optimizer Memory Analysis (per parameter tensor):\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'Size':<10} {'SGD':<10} {'Adam':<10} {'AdamW':<10} {'Ratio':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for size in param_sizes:\n",
    "        # Create parameter\n",
    "        param = Tensor(np.random.randn(size), requires_grad=True)\n",
    "        param.grad = Tensor(np.random.randn(size))\n",
    "\n",
    "        # SGD memory (parameter + momentum buffer)\n",
    "        sgd = SGD([param], momentum=0.9)\n",
    "        sgd.step()  # Initialize buffers\n",
    "        sgd_memory = size * 2  # param + momentum buffer\n",
    "\n",
    "        # Adam memory (parameter + 2 moment buffers)\n",
    "        param_adam = Tensor(np.random.randn(size), requires_grad=True)\n",
    "        param_adam.grad = Tensor(np.random.randn(size))\n",
    "        adam = Adam([param_adam])\n",
    "        adam.step()  # Initialize buffers\n",
    "        adam_memory = size * 3  # param + m_buffer + v_buffer\n",
    "\n",
    "        # AdamW memory (same as Adam)\n",
    "        adamw_memory = adam_memory\n",
    "\n",
    "        # Memory ratio (Adam/SGD)\n",
    "        ratio = adam_memory / sgd_memory\n",
    "\n",
    "        print(f\"{size:<10} {sgd_memory:<10} {adam_memory:<10} {adamw_memory:<10} {ratio:.1f}x\")\n",
    "\n",
    "    print(\"\\nðŸ’¡ Key Insights:\")\n",
    "    print(\"- SGD: 2Ã— parameter memory (momentum buffer)\")\n",
    "    print(\"- Adam/AdamW: 3Ã— parameter memory (two moment buffers)\")\n",
    "    print(\"- Memory scales linearly with model size\")\n",
    "    print(\"- Trade-off: More memory for better convergence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71c41a7d-03e8-475b-9a14-eb10975074db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_optimizer_convergence_behavior():\n",
    "    \"\"\"ðŸ“Š Analyze convergence behavior of different optimizers.\"\"\"\n",
    "    print(\"ðŸ“Š Analyzing Optimizer Convergence Behavior...\")\n",
    "\n",
    "    # Simulate optimization of a quadratic function: f(x) = 0.5 * x^2\n",
    "    # Optimal solution: x* = 0, gradient = x\n",
    "\n",
    "    def quadratic_loss(x):\n",
    "        \"\"\"Simple quadratic function for optimization testing.\"\"\"\n",
    "        return 0.5 * (x ** 2).sum()\n",
    "\n",
    "    def compute_gradient(x):\n",
    "        \"\"\"Gradient of quadratic function: df/dx = x.\"\"\"\n",
    "        return x.copy()\n",
    "\n",
    "    # Starting point\n",
    "    x_start = np.array([5.0, -3.0, 2.0])  # Far from optimum [0, 0, 0]\n",
    "\n",
    "    # Test different optimizers\n",
    "    optimizers_to_test = [\n",
    "        (\"SGD\", SGD, {\"lr\": 0.1}),\n",
    "        (\"SGD+Momentum\", SGD, {\"lr\": 0.1, \"momentum\": 0.9}),\n",
    "        (\"Adam\", Adam, {\"lr\": 0.1}),\n",
    "        (\"AdamW\", AdamW, {\"lr\": 0.1, \"weight_decay\": 0.01})\n",
    "    ]\n",
    "\n",
    "    print(\"Convergence Analysis (quadratic function f(x) = 0.5 * xÂ²):\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Optimizer':<15} {'Step 0':<12} {'Step 5':<12} {'Step 10':<12} {'Final Loss':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    for name, optimizer_class, kwargs in optimizers_to_test:\n",
    "        # Reset parameter\n",
    "        param = Tensor(x_start.copy(), requires_grad=True)\n",
    "        optimizer = optimizer_class([param], **kwargs)\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        # Run optimization for 10 steps\n",
    "        for step in range(11):\n",
    "            # Compute loss and gradient\n",
    "            loss = quadratic_loss(param.data)\n",
    "            param.grad = Tensor(compute_gradient(param.data))\n",
    "\n",
    "            losses.append(loss)\n",
    "\n",
    "            # Update parameters\n",
    "            if step < 10:  # Don't update after last evaluation\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        # Format results\n",
    "        step0 = f\"{losses[0]:.6f}\"\n",
    "        step5 = f\"{losses[5]:.6f}\"\n",
    "        step10 = f\"{losses[10]:.6f}\"\n",
    "        final = f\"{losses[10]:.6f}\"\n",
    "\n",
    "        print(f\"{name:<15} {step0:<12} {step5:<12} {step10:<12} {final:<12}\")\n",
    "\n",
    "    print(\"\\nðŸ’¡ Key Insights:\")\n",
    "    print(\"- SGD: Steady progress but can be slow\")\n",
    "    print(\"- SGD+Momentum: Faster convergence, less oscillation\")\n",
    "    print(\"- Adam: Adaptive rates help with different parameter scales\")\n",
    "    print(\"- AdamW: Similar to Adam with regularization effects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2104b0ba-bbe2-4ad1-8359-a34c36ded5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_module():\n",
    "    \"\"\"ðŸ§ª Module Test: Complete Integration\n",
    "\n",
    "    Comprehensive test of entire module functionality.\n",
    "\n",
    "    This final test runs before module summary to ensure:\n",
    "    - All unit tests pass\n",
    "    - Functions work together correctly\n",
    "    - Module is ready for integration with TinyTorch\n",
    "    \"\"\"\n",
    "    print(\"ðŸ§ª RUNNING MODULE INTEGRATION TEST\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Run all unit tests\n",
    "    print(\"Running unit tests...\")\n",
    "    test_unit_optimizer_base()\n",
    "    test_unit_sgd_optimizer()\n",
    "    test_unit_adam_optimizer()\n",
    "    test_unit_adamw_optimizer()\n",
    "\n",
    "    print(\"\\nRunning integration scenarios...\")\n",
    "\n",
    "    # Test realistic neural network optimization scenario\n",
    "    print(\"ðŸ”¬ Integration Test: Multi-layer Network Optimization...\")\n",
    "\n",
    "    # Import components from TinyTorch package (previous modules must be completed and exported)\n",
    "    from tinytorch.core.layers import Linear\n",
    "    from tinytorch.core.activations import ReLU\n",
    "    from tinytorch.core.losses import MSELoss\n",
    "\n",
    "    # Create parameters for a 2-layer network\n",
    "    # Layer 1: 3 inputs -> 4 hidden\n",
    "    W1 = Tensor(np.random.randn(3, 4) * 0.1, requires_grad=True)\n",
    "    b1 = Tensor(np.zeros(4), requires_grad=True)\n",
    "\n",
    "    # Layer 2: 4 hidden -> 2 outputs\n",
    "    W2 = Tensor(np.random.randn(4, 2) * 0.1, requires_grad=True)\n",
    "    b2 = Tensor(np.zeros(2), requires_grad=True)\n",
    "\n",
    "    params = [W1, b1, W2, b2]\n",
    "\n",
    "    # Add realistic gradients\n",
    "    W1.grad = Tensor(np.random.randn(3, 4) * 0.01)\n",
    "    b1.grad = Tensor(np.random.randn(4) * 0.01)\n",
    "    W2.grad = Tensor(np.random.randn(4, 2) * 0.01)\n",
    "    b2.grad = Tensor(np.random.randn(2) * 0.01)\n",
    "\n",
    "    # Test all optimizers on same network\n",
    "    optimizers = [\n",
    "        SGD(params, lr=0.01, momentum=0.9),\n",
    "        Adam([p for p in params], lr=0.001),  # Fresh param list for Adam\n",
    "        AdamW([p for p in params], lr=0.001, weight_decay=0.01)  # Fresh param list for AdamW\n",
    "    ]\n",
    "\n",
    "    # Save original parameter values\n",
    "    original_params = [p.data.copy() for p in params]\n",
    "\n",
    "    # Test SGD\n",
    "    optimizers[0].step()\n",
    "    sgd_params = [p.data.copy() for p in params]\n",
    "\n",
    "    # Restore parameters and test Adam\n",
    "    for i, p in enumerate(params):\n",
    "        p.data = original_params[i].copy()\n",
    "        # Re-add gradients since they may have been modified\n",
    "        if i == 0:\n",
    "            p.grad = Tensor(np.random.randn(3, 4) * 0.01)\n",
    "        elif i == 1:\n",
    "            p.grad = Tensor(np.random.randn(4) * 0.01)\n",
    "        elif i == 2:\n",
    "            p.grad = Tensor(np.random.randn(4, 2) * 0.01)\n",
    "        else:\n",
    "            p.grad = Tensor(np.random.randn(2) * 0.01)\n",
    "\n",
    "    # Update parameter references for Adam\n",
    "    optimizers[1].params = params\n",
    "    optimizers[1].step()\n",
    "    adam_params = [p.data.copy() for p in params]\n",
    "\n",
    "    # Restore parameters and test AdamW\n",
    "    for i, p in enumerate(params):\n",
    "        p.data = original_params[i].copy()\n",
    "        # Re-add gradients\n",
    "        if i == 0:\n",
    "            p.grad = Tensor(np.random.randn(3, 4) * 0.01)\n",
    "        elif i == 1:\n",
    "            p.grad = Tensor(np.random.randn(4) * 0.01)\n",
    "        elif i == 2:\n",
    "            p.grad = Tensor(np.random.randn(4, 2) * 0.01)\n",
    "        else:\n",
    "            p.grad = Tensor(np.random.randn(2) * 0.01)\n",
    "\n",
    "    # Update parameter references for AdamW\n",
    "    optimizers[2].params = params\n",
    "    optimizers[2].step()\n",
    "    adamw_params = [p.data.copy() for p in params]\n",
    "\n",
    "    # Verify parameters changed differently for each optimizer\n",
    "    for i in range(len(params)):\n",
    "        # Parameters should be different from original\n",
    "        assert not np.array_equal(sgd_params[i], original_params[i])\n",
    "        assert not np.array_equal(adam_params[i], original_params[i])\n",
    "        assert not np.array_equal(adamw_params[i], original_params[i])\n",
    "\n",
    "        # Different optimizers should produce different results\n",
    "        assert not np.allclose(sgd_params[i], adam_params[i], rtol=1e-6)\n",
    "\n",
    "    print(\"âœ… Multi-layer network optimization works!\")\n",
    "\n",
    "    # Test optimizer state management\n",
    "    print(\"ðŸ”¬ Integration Test: Optimizer State Management...\")\n",
    "\n",
    "    param = Tensor([1.0, 2.0], requires_grad=True)\n",
    "    param.grad = Tensor([0.1, 0.2])\n",
    "\n",
    "    optimizer = Adam([param], lr=0.001)\n",
    "\n",
    "    # First step should initialize buffers\n",
    "    optimizer.step()\n",
    "    assert optimizer.m_buffers[0] is not None\n",
    "    assert optimizer.v_buffers[0] is not None\n",
    "    assert optimizer.step_count == 1\n",
    "\n",
    "    # Zero grad should clear gradients but preserve optimizer state\n",
    "    optimizer.zero_grad()\n",
    "    assert param.grad is None\n",
    "    assert optimizer.m_buffers[0] is not None  # State preserved\n",
    "    assert optimizer.step_count == 1  # Step count preserved\n",
    "\n",
    "    print(\"âœ… Optimizer state management works!\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ðŸŽ‰ ALL TESTS PASSED! Module ready for export.\")\n",
    "    print(\"Run: tito module complete 07_optimizers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "846ebcd9-3ef0-420b-9f15-01bba3f5eec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_optimizers():\n",
    "    \"\"\"ðŸŽ¯ See optimizers update weights.\"\"\"\n",
    "    print(\"ðŸŽ¯ AHA MOMENT: Optimizers Update Weights\")\n",
    "    print(\"=\" * 45)\n",
    "\n",
    "    # Create a parameter with a gradient\n",
    "    weight = Tensor(np.array([5.0]), requires_grad=True)\n",
    "    weight.grad = np.array([1.0])  # Gradient pointing \"uphill\"\n",
    "\n",
    "    print(f\"Initial weight: {weight.data[0]:.2f}\")\n",
    "    print(f\"Gradient:       {weight.grad[0]:.2f} (pointing uphill)\")\n",
    "\n",
    "    # SGD takes a step in the opposite direction\n",
    "    optimizer = SGD([weight], lr=0.5)\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"\\nAfter SGD step: {weight.data[0]:.2f}\")\n",
    "    print(f\"Moved: {5.0 - weight.data[0]:.2f} (opposite to gradient)\")\n",
    "\n",
    "    print(\"\\nâœ¨ Optimizer moves weights to reduce loss!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64c51c55-3763-420c-8176-d983c280bdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª RUNNING MODULE INTEGRATION TEST\n",
      "==================================================\n",
      "Running unit tests...\n",
      "ðŸ”¬ Unit Test: Base Optimizer...\n",
      "âœ… Base Optimizer works correctly!\n",
      "ðŸ”¬ Unit Test: SGD Optimizer...\n",
      "âœ… SGD optimizer works correctly!\n",
      "ðŸ”¬ Unit Test: Adam Optimizer...\n",
      "âœ… Adam optimizer works correctly!\n",
      "ðŸ”¬ Unit Test: AdamW Optimizer...\n",
      "âœ… AdamW optimizer works correctly!\n",
      "\n",
      "Running integration scenarios...\n",
      "ðŸ”¬ Integration Test: Multi-layer Network Optimization...\n",
      "âœ… Multi-layer network optimization works!\n",
      "ðŸ”¬ Integration Test: Optimizer State Management...\n",
      "âœ… Optimizer state management works!\n",
      "\n",
      "==================================================\n",
      "ðŸŽ‰ ALL TESTS PASSED! Module ready for export.\n",
      "Run: tito module complete 07_optimizers\n",
      "\n",
      "\n",
      "ðŸŽ¯ AHA MOMENT: Optimizers Update Weights\n",
      "=============================================\n",
      "Initial weight: 5.00\n",
      "Gradient:       1.00 (pointing uphill)\n",
      "\n",
      "After SGD step: 4.50\n",
      "Moved: 0.50 (opposite to gradient)\n",
      "\n",
      "âœ¨ Optimizer moves weights to reduce loss!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test_module()\n",
    "    print(\"\\n\")\n",
    "    demo_optimizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c49b69-5ed1-41bc-9c34-c86650b001ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
