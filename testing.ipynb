{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9198587b-5fe1-4ee6-ba05-06a63952acd0",
   "metadata": {},
   "source": [
    "<span style=' font-size:20px'>Arithmetic Computation Testing</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b39aab0-c3ef-46c8-8f6f-17b4f622f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.core.tensor import Tensor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7b64dd-efa0-42ef-ae11-4cefc280fb6a",
   "metadata": {},
   "source": [
    "## Elementwise Arithemtic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1a31b959-de42-4a71-bc8a-17d8098d0e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Unit Test: Arithmetic Operations...\n",
      "âœ… Arithmetic operations work correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_arithmetic_operations():\n",
    "    print(\"ðŸ§ª Unit Test: Arithmetic Operations...\")\n",
    "\n",
    "    # Test tensor + tensor\n",
    "    a = Tensor([1, 2, 3])\n",
    "    b = Tensor([4, 5, 6])\n",
    "    result = a + b\n",
    "    assert np.array_equal(result.data, np.array([5, 7, 9], dtype=np.float32))\n",
    "\n",
    "    # Test tensor + scaler\n",
    "    result = a + 10\n",
    "    assert np.array_equal(result.data, np.array([11, 12, 13], dtype=np.float32))\n",
    "\n",
    "    # testing broadcasting with different shapes(matrix + vector)\n",
    "    matrix = Tensor([[1, 2], [3, 4]])\n",
    "    vector = Tensor([10, 20])\n",
    "    result = matrix + vector\n",
    "    expected = np.array([[11, 22], [13, 24]], dtype=np.float32)\n",
    "    assert np.array_equal(result.data, expected)\n",
    "\n",
    "    # Test subtraction (data centering)\n",
    "    result = b - a\n",
    "    assert np.array_equal(result.data, np.array([3, 3, 3], dtype=np.float32))\n",
    "\n",
    "    # Test multiplication (scaling)\n",
    "    result = a * 2\n",
    "    assert np.array_equal(result.data, np.array([2, 4, 6], dtype=np.float32))\n",
    "\n",
    "    # Test division \n",
    "    normalized = (a - 2) / 2 # center and scale\n",
    "    expected = np.array([-0.5, 0.0, 0.5], dtype=np.float32)\n",
    "    assert np.allclose(normalized.data, expected)\n",
    "    \n",
    "    print(\"âœ… Arithmetic operations work correctly!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_unit_arithmetic_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b1d58f-6212-4dea-94d9-eee0f57fdcd5",
   "metadata": {},
   "source": [
    "## Matrix Multiplcation The Heart of Neural Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c21680bb-d798-4651-9234-4ef2bf35c0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Unit Test: Matrix Multiplication...\n",
      "âœ… Shape manipulation works correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_matrix_multiplication():\n",
    "    \"\"\"ðŸ§ª Test matrix multiplication operations.\"\"\"\n",
    "    print(\"ðŸ§ª Unit Test: Matrix Multiplication...\")\n",
    "\n",
    "    # test 2 x 2 matrix multiplication\n",
    "    a = Tensor([[1, 2], [3, 4]])\n",
    "    b = Tensor([[5, 6], [7, 8]])\n",
    "    result = a.matmul(b)\n",
    "    expected = np.array([[19, 22], [43, 50]], dtype=np.float32)\n",
    "    assert np.array_equal(result.data, expected)\n",
    "\n",
    "    # Test rectangular matrices(common in nueral networks)\n",
    "    c = Tensor([[1, 2 , 3], [4, 5, 6]])\n",
    "    d = Tensor([[7, 8], [9, 10], [11, 12]])\n",
    "    result = c.matmul(d)\n",
    "    expected = np.array([[58, 64], [139, 154]], dtype=np.float32)\n",
    "    assert np.array_equal(result.data, expected)\n",
    "\n",
    "    # matrix vector multiplication\n",
    "    matrix = Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "    vector = Tensor([1, 2, 3])\n",
    "    result = matrix.matmul(vector)\n",
    "    expected = np.array([14, 32])\n",
    "    assert np.array_equal(result.data, expected)\n",
    "\n",
    "    # Test shape validation - should raise clear errors\n",
    "    try:\n",
    "        incompatible_a = Tensor([[1, 2]])\n",
    "        incompatible_b = Tensor([[1], [2], [3]])\n",
    "        incompatible_a.matmul(incompatible_b)\n",
    "        assert False, \"Should have raised Value Error for incompatible shapes\"\n",
    "    except ValueError as e:\n",
    "        assert \"Inner dimensions must match\" in str(e)\n",
    "        assert \"2 â‰  3\" in str(e)\n",
    "\n",
    "    print(\"âœ… Shape manipulation works correctly!\")\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_unit_matrix_multiplication()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4e3e58-1464-4a3c-a84e-997845e192f8",
   "metadata": {},
   "source": [
    "## Shape Manipulation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d3b68099-546e-4f26-bc08-af8b5673c92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Unit Test: Shape Manipulation...\n",
      "âœ… Shape manipulation works correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_matrix_multiplication():\n",
    "    \"\"\"ðŸ§ª Test reshape and transpose operations.\"\"\"\n",
    "    print(\"ðŸ§ª Unit Test: Shape Manipulation...\")\n",
    "\n",
    "    # Test basic reshape (flatten => matrix)\n",
    "    tensor = Tensor([1, 2, 3, 4, 5, 6])\n",
    "    reshaped = tensor.reshape(2, 3)\n",
    "    assert reshaped.shape == (2, 3)\n",
    "    expected = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n",
    "    assert np.array_equal(reshaped.data, expected)\n",
    "\n",
    "    # Test reshape with tuple (aternative calling style)\n",
    "    reshaped2 = tensor.reshape((3, 2))\n",
    "    assert reshaped2.shape == (3, 2)\n",
    "    expected = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "    assert np.array_equal(reshaped2.data, expected)\n",
    "\n",
    "    # Test reshape with -1 (automatic dimension inference)\n",
    "    auto_reshaped = tensor.reshape(2, -1)\n",
    "    assert auto_reshaped.shape == (2, 3)\n",
    "\n",
    "    # Test reshape validation - should raise error for incompatible sizes\n",
    "    try:\n",
    "        tensor.reshape(2, 2)\n",
    "        assert False, 'Should have raised ValueError'\n",
    "    except ValueError as e:\n",
    "        assert \"Total elements must match\" in str(e)\n",
    "        assert \"6 â‰  4\" in str(e)\n",
    "\n",
    "    # Test matrix transpose (most common case)\n",
    "    matrix = Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "    transposed = matrix.transpose()\n",
    "    assert transposed.shape == (3, 2)\n",
    "    expected = np.array([[1, 4], [2, 5], [3, 6]], dtype=np.float32)\n",
    "    assert np.array_equal(transposed.data, expected)\n",
    "\n",
    "    # test 1D transpose (should be identity)\n",
    "    vector = Tensor([1, 2, 3])\n",
    "    vector_t = vector.transpose()\n",
    "    assert np.array_equal(vector.data, vector_t.data)\n",
    "\n",
    "    # test specific dimensions transpose\n",
    "    tensor_3d = Tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]) \n",
    "    swapped = tensor_3d.transpose(0, 2)\n",
    "    assert swapped.shape == (2, 2, 2)\n",
    "\n",
    "    # Test neural network reshape pattern (flatten for MLP)\n",
    "    batch_images = Tensor(np.random.randn(2, 3, 4)) # (batch= 2, height= 3, width = 4)\n",
    "    flattened = batch_images.reshape(2, -1) # (batch=2, features= 12)\n",
    "    assert flattened.shape == (2, 12)\n",
    "        \n",
    "    print(\"âœ… Shape manipulation works correctly!\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    test_unit_matrix_multiplication()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129aca09-3d0b-4e6e-b126-d5df6f74fefa",
   "metadata": {},
   "source": [
    "## Reduction Operations: Aggregation Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "736f365d-8719-41ae-aa04-bfde5ae037af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Unit Test: Reduction Operations...\n",
      "âœ… Reduction operations work correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_shape_manipulation():\n",
    "    \"\"\"ðŸ§ª Test reduction operations.\"\"\"\n",
    "    print(\"ðŸ§ª Unit Test: Reduction Operations...\")\n",
    "\n",
    "    matrix = Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "    # test sum all elements\n",
    "    total = matrix.sum()\n",
    "    assert total.data == 21.0\n",
    "    assert total.shape == ()\n",
    "\n",
    "    # Test sum along axis 0 (colums) - batch dimension reduction \n",
    "    col_sum = matrix.sum(axis= 0)\n",
    "    expected_col = np.array([5, 7, 9], dtype= np.float32)\n",
    "    assert np.array_equal(col_sum.data, expected_col)\n",
    "    assert col_sum.shape == (3, )\n",
    "\n",
    "    # Test sum along axis 1 (row) - feature dimesnion reduction\n",
    "    row_sum= matrix.sum(1)\n",
    "    expected_row = np.array([6, 15], dtype=np.float32)\n",
    "    assert row_sum.shape == (2, )\n",
    "\n",
    "    # Test mean (average loss computation)\n",
    "    avg = matrix.mean()\n",
    "    assert np.isclose(avg.data, 3.5)\n",
    "    assert avg.shape == ()\n",
    "\n",
    "    # Test mean along axis (batch normalization pattern)\n",
    "    col_mean = matrix.mean(axis=0)\n",
    "    expected_mean = np.array([2.5, 3.5, 4.5], dtype=np.float32)  # [5/2, 7/2, 9/2]\n",
    "    assert np.allclose(col_mean.data, expected_mean)\n",
    "\n",
    "    # Test max (finding best predictions)\n",
    "    maximum = matrix.max()\n",
    "    assert maximum.data == 6.0\n",
    "    assert maximum.shape == ()\n",
    "\n",
    "    # Test max along axis (argmax-like operation)\n",
    "    row_max = matrix.max(axis=1)\n",
    "    expected_max = np.array([3, 6], dtype=np.float32)  # [max(1,2,3), max(4,5,6)]\n",
    "    assert np.array_equal(row_max.data, expected_max)\n",
    "\n",
    "    # Test keepdims (important for broadcasting)\n",
    "    sum_keepdims = matrix.sum(axis=1, keepdims=True)\n",
    "    assert sum_keepdims.shape == (2, 1) \n",
    "    expected_keepdims = np.array([[6], [15]], dtype=np.float32)\n",
    "    assert np.array_equal(sum_keepdims.data, expected_keepdims)\n",
    "\n",
    "    # Test 3D reduction (simulating global average pooling)\n",
    "    tensor_3d = Tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])  # (2, 2, 2)\n",
    "    spatial_mean = tensor_3d.mean(axis=(1, 2))  # Average across spatial dimensions\n",
    "    assert spatial_mean.shape == (2,)  # One value per batch item\n",
    "    \n",
    "\n",
    "    print(\"âœ… Reduction operations work correctly!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_unit_shape_manipulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0698f80a-8fe3-4704-8fda-494efc8420e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Analyzing Memory Access Patterns...\n",
      "============================================================\n",
      "\n",
      "Testing with 2000x2000 matrix (15.3MB)\n",
      "------------------------------------------------------------\n",
      "Test 1: Row-wise Access (Cache friendly)\n",
      "  Time: 17.0ms\n",
      "  Access pattern: sequential (follows memory layout)\n",
      "\n",
      "Test 2: Column-wise Access (Cache unfriendly)\n",
      "  Time: 17.0ms\n",
      "  Access pattern: Strided (jumps 8000 bytes per element)\n",
      "\n",
      "============================================================\n",
      "ðŸ“Š PERFORMANCE IMPACT:\n",
      "   Slowdown factor: 2.93Ã— (2.9Ã— slower)\n",
      "   Cache misses cause 193% performance loss\n",
      "\n",
      "ðŸ’¡ KEY INSIGHTS:\n",
      "   1. Memory layout matters: Row-major (C-style) storage is sequential\n",
      "   2. Cache lines are ~64 bytes: Row access loads nearby elements \"for free\"\n",
      "   3. Column access misses cache: Must reload from DRAM every time\n",
      "   4. This is O(n) algorithm but 2.9Ã— different wall-clock time!\n",
      "\n",
      "ðŸš€ REAL-WORLD IMPLICATIONS:\n",
      "   â€¢ CNNs use NCHW format (channels sequential) for cache efficiency\n",
      "   â€¢ Matrix multiplication optimized with blocking (tile into cache-sized chunks)\n",
      "   â€¢ Transpose is expensive (2.9Ã—) because it changes memory layout\n",
      "   â€¢ This is why GPU frameworks obsess over memory coalescing\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "BYTES_PER_FLOAT32 = 4\n",
    "KB_TO_BYTES = 1024\n",
    "MB_TO_BYTES = 1024 * 1024\n",
    "\n",
    "def analyze_memory_layout():\n",
    "    \"\"\"ðŸ“Š Demonstrate cache effects with row vs column access patterns.\"\"\"\n",
    "    print(\"ðŸ“Š Analyzing Memory Access Patterns...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    import time \n",
    "    \n",
    "    size = 2000\n",
    "    matrix = Tensor(np.random.rand(size, size))\n",
    "\n",
    "    import time\n",
    "    print(f'\\nTesting with {size}x{size} matrix ({matrix.size * BYTES_PER_FLOAT32 / MB_TO_BYTES:.1f}MB)')\n",
    "    print('-'*60)\n",
    "    print(f'Test 1: Row-wise Access (Cache friendly)')\n",
    "    start = time.time()\n",
    "    row_sums = []\n",
    "    for i in range(size):\n",
    "        row_sum = matrix.data[i, :].sum() # access entire row sequentially\n",
    "        row_sums.append(row_sum)\n",
    "    row_time = time.time() - start\n",
    "    print(f\"  Time: {row_time*1000:.1f}ms\")\n",
    "    print(f\"  Access pattern: sequential (follows memory layout)\")\n",
    "\n",
    "    print(f'\\nTest 2: Column-wise Access (Cache unfriendly)')\n",
    "    start = time.time()\n",
    "    col_sums = []\n",
    "    for i in range(size):\n",
    "        col_sum = matrix.data[:, i].sum()\n",
    "        col_sums.append(col_sum)\n",
    "    col_time = time.time() - start\n",
    "    print(f\"  Time: {row_time*1000:.1f}ms\")\n",
    "    print(f\"  Access pattern: Strided (jumps {size * BYTES_PER_FLOAT32} bytes per element)\")\n",
    "    \n",
    "    # Calculate slowdown\n",
    "    slowdown = col_time / row_time\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"ðŸ“Š PERFORMANCE IMPACT:\")\n",
    "    print(f\"   Slowdown factor: {slowdown:.2f}Ã— ({col_time/row_time:.1f}Ã— slower)\")\n",
    "    print(f\"   Cache misses cause {(slowdown-1)*100:.0f}% performance loss\")\n",
    "\n",
    "    # Educational insights\n",
    "    print(\"\\nðŸ’¡ KEY INSIGHTS:\")\n",
    "    print(f\"   1. Memory layout matters: Row-major (C-style) storage is sequential\")\n",
    "    print(f\"   2. Cache lines are ~64 bytes: Row access loads nearby elements \\\"for free\\\"\")\n",
    "    print(f\"   3. Column access misses cache: Must reload from DRAM every time\")\n",
    "    print(f\"   4. This is O(n) algorithm but {slowdown:.1f}Ã— different wall-clock time!\")\n",
    "\n",
    "    print(\"\\nðŸš€ REAL-WORLD IMPLICATIONS:\")\n",
    "    print(f\"   â€¢ CNNs use NCHW format (channels sequential) for cache efficiency\")\n",
    "    print(f\"   â€¢ Matrix multiplication optimized with blocking (tile into cache-sized chunks)\")\n",
    "    print(f\"   â€¢ Transpose is expensive ({slowdown:.1f}Ã—) because it changes memory layout\")\n",
    "    print(f\"   â€¢ This is why GPU frameworks obsess over memory coalescing\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    analyze_memory_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5919a854-280a-47fa-8c33-f7eea4e30577",
   "metadata": {},
   "source": [
    "## Bringing It all Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "13823082-34ea-4062-8dd8-533a4052ad97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª RUNNING MODULE INTEGRATION TEST\n",
      "==================================================\n",
      "Running unit tests...\n",
      "\n",
      "Running integration scenarios...\n",
      "ðŸ§ª Integration Test: Two-Layer Neural Network...\n",
      "âœ… Two-layer neural network computation works!\n",
      "ðŸ§ª Integration Test: Complex Shape Operations...\n",
      "âœ… Complex shape operations work!\n",
      "ðŸ§ª Integration Test: Broadcasting Edge Cases...\n",
      "âœ… Broadcasting edge cases work!\n",
      "\n",
      "==================================================\n",
      "ðŸŽ‰ ALL TESTS PASSED! Module ready for export.\n",
      "Run: tito module complete 01_tensor\n"
     ]
    }
   ],
   "source": [
    "def test_module():\n",
    "    \"\"\"ðŸ§ª Module Test: Complete Integration\n",
    "\n",
    "    Comprehensive test of entire module functionality.\n",
    "\n",
    "    This final test runs before module summary to ensure:\n",
    "    - All unit tests pass\n",
    "    - Functions work together correctly\n",
    "    - Module is ready for integration with TinyTorch\n",
    "    \"\"\"\n",
    "    print(\"ðŸ§ª RUNNING MODULE INTEGRATION TEST\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Run all unit tests\n",
    "    print(\"Running unit tests...\")\n",
    "    # test_unit_tensor_creation()\n",
    "    # test_unit_arithmetic_operations()\n",
    "    # test_unit_matrix_multiplication()\n",
    "    # test_unit_shape_manipulation()\n",
    "    # test_unit_reduction_operations()\n",
    "\n",
    "    print(\"\\nRunning integration scenarios...\")\n",
    "\n",
    "    # Test realistic neural network computation\n",
    "    print(\"ðŸ§ª Integration Test: Two-Layer Neural Network...\")\n",
    "\n",
    "    # Create input data (2 samples, 3 features)\n",
    "    x = Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "    # First layer: 3 inputs â†’ 4 hidden units\n",
    "    W1 = Tensor([[0.1, 0.2, 0.3, 0.4],\n",
    "                 [0.5, 0.6, 0.7, 0.8],\n",
    "                 [0.9, 1.0, 1.1, 1.2]])\n",
    "    b1 = Tensor([0.1, 0.2, 0.3, 0.4])\n",
    "\n",
    "    # Forward pass: hidden = xW1 + b1\n",
    "    hidden = x.matmul(W1) + b1\n",
    "    assert hidden.shape == (2, 4), f\"Expected (2, 4), got {hidden.shape}\"\n",
    "\n",
    "    # Second layer: 4 hidden â†’ 2 outputs\n",
    "    W2 = Tensor([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6], [0.7, 0.8]])\n",
    "    b2 = Tensor([0.1, 0.2])\n",
    "\n",
    "    # Output layer: output = hiddenW2 + b2\n",
    "    output = hidden.matmul(W2) + b2\n",
    "    assert output.shape == (2, 2), f\"Expected (2, 2), got {output.shape}\"\n",
    "\n",
    "    # Verify data flows correctly (no NaN, reasonable values)\n",
    "    assert not np.isnan(output.data).any(), \"Output contains NaN values\"\n",
    "    assert np.isfinite(output.data).all(), \"Output contains infinite values\"\n",
    "\n",
    "    print(\"âœ… Two-layer neural network computation works!\")\n",
    "\n",
    "    # Test complex shape manipulations\n",
    "    print(\"ðŸ§ª Integration Test: Complex Shape Operations...\")\n",
    "    data = Tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n",
    "\n",
    "    # Reshape to 3D tensor (simulating batch processing)\n",
    "    tensor_3d = data.reshape(2, 2, 3)  # (batch=2, height=2, width=3)\n",
    "    assert tensor_3d.shape == (2, 2, 3)\n",
    "\n",
    "    # Global average pooling simulation\n",
    "    pooled = tensor_3d.mean(axis=(1, 2))  # Average across spatial dimensions\n",
    "    assert pooled.shape == (2,), f\"Expected (2,), got {pooled.shape}\"\n",
    "\n",
    "    # Flatten for MLP\n",
    "    flattened = tensor_3d.reshape(2, -1)  # (batch, features)\n",
    "    assert flattened.shape == (2, 6)\n",
    "\n",
    "    # Transpose for different operations\n",
    "    transposed = tensor_3d.transpose()  # Should transpose last two dims\n",
    "    assert transposed.shape == (2, 3, 2)\n",
    "\n",
    "    print(\"âœ… Complex shape operations work!\")\n",
    "\n",
    "    # Test broadcasting edge cases\n",
    "    print(\"ðŸ§ª Integration Test: Broadcasting Edge Cases...\")\n",
    "\n",
    "    # Scalar broadcasting\n",
    "    scalar = Tensor(5.0)\n",
    "    vector = Tensor([1, 2, 3])\n",
    "    result = scalar + vector  # Should broadcast scalar to vector shape\n",
    "    expected = np.array([6, 7, 8], dtype=np.float32)\n",
    "    assert np.array_equal(result.data, expected)\n",
    "\n",
    "    # Matrix + vector broadcasting\n",
    "    matrix = Tensor([[1, 2], [3, 4]])\n",
    "    vec = Tensor([10, 20])\n",
    "    result = matrix + vec\n",
    "    expected = np.array([[11, 22], [13, 24]], dtype=np.float32)\n",
    "    assert np.array_equal(result.data, expected)\n",
    "\n",
    "    print(\"âœ… Broadcasting edge cases work!\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ðŸŽ‰ ALL TESTS PASSED! Module ready for export.\")\n",
    "    print(\"Run: tito module complete 01_tensor\")\n",
    "\n",
    "# Run comprehensive module test\n",
    "if __name__ == \"__main__\":\n",
    "    test_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8d8870-0c86-4620-a446-4dc7b4a3a399",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
