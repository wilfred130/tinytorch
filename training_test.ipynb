{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5132678e-ff34-4c32-9c94-c85434939bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "\n",
    "from tinytorch.core.tensor import Tensor\n",
    "from tinytorch.core.training import CosineSchedule, clip_grad_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b65e0cd-d847-4120-bd82-933696b89325",
   "metadata": {},
   "source": [
    "# Unit Test - CosineSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dafd3117-1c59-4dd9-a3e5-380bb91da657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Unit Test: CosineSchedule...\n",
      "    Learning rate at epoch 0: 0.1000\n",
      "    Learning rate at epoch 50: 0.0550\n",
      "    Learning rate at epoch 100: 0.0100\n",
      "âœ… CosineSchedule works correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_cosine_schedule():\n",
    "    print(\"ðŸ”¬ Unit Test: CosineSchedule...\")\n",
    "\n",
    "    # test basic schedule\n",
    "    schedule = CosineSchedule(max_lr= 0.1, min_lr= 0.01, total_epochs=100)\n",
    "\n",
    "    # Test start, middle, and end\n",
    "    lr_start = schedule.get_lr(0)\n",
    "    lr_middle = schedule.get_lr(50)\n",
    "    lr_end = schedule.get_lr(100)\n",
    "\n",
    "    print(f\"    Learning rate at epoch 0: {lr_start:.4f}\")\n",
    "    print(f\"    Learning rate at epoch 50: {lr_middle:.4f}\")\n",
    "    print(f\"    Learning rate at epoch 100: {lr_end:.4f}\")\n",
    "\n",
    "    # Validate behavior\n",
    "    assert abs(lr_start - 0.1) < 1e-6, f\"Expected 0.1 at start, got {lr_start}\"\n",
    "    assert abs(lr_end - 0.01) < 1e-6, f\"Expedted 0.01 at end, got {lr_end}\"\n",
    "    assert 0.01 < lr_middle < 0.1, f\"Middle LR should be between min and max, got {lr_middle}\"\n",
    "\n",
    "    # Test monotonic decrease in first half\n",
    "    lr_quarter = schedule.get_lr(25)\n",
    "    assert lr_quarter > lr_middle, \"LR should decrease monotonically in first half\"\n",
    "\n",
    "    print(\"âœ… CosineSchedule works correctly!\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    test_unit_cosine_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b93f53f-729a-4ae1-9bbe-f08a332e758a",
   "metadata": {},
   "source": [
    "## Unit Test - Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84c2ce92-b2d2-43d7-9320-880e15793b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Unit Test: Gradient Clipping...\n",
      "Original norm: 11.18\n",
      "Clipped norm: 1.00\n",
      "âœ… Gradient clipping works correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_clip_grad_norm():\n",
    "    print(\"ðŸ”¬ Unit Test: Gradient Clipping...\")\n",
    "\n",
    "    import sys\n",
    "    \n",
    "    param1 = Tensor([1.0, 2.0], requires_grad= True)\n",
    "    param1.grad = np.array([3.0, 4.0]) # norm = 5.0\n",
    "\n",
    "    param2 = Tensor([3.0, 4.0], requires_grad= True)\n",
    "    param2.grad = np.array([6.0, 8.0]) # norm = 10.0\n",
    "\n",
    "    params = [param1, param2]\n",
    "    # Total norm = sqrt (25 + 100) = 11.18\n",
    "\n",
    "    original_norm = clip_grad_norm(params, max_norm=1.0)\n",
    "\n",
    "    assert original_norm > 1.0, f\"Original norm should be > 1.0,  got {original_norm}\"\n",
    "\n",
    "    # check if gradients were clipped\n",
    "    new_norm = 0.0\n",
    "    for param in params:\n",
    "        if isinstance(param.grad, np.ndarray):\n",
    "            grad_data = param.grad\n",
    "        else:\n",
    "            grad_data = param.grad.data\n",
    "\n",
    "        new_norm += np.sum(grad_data ** 2)\n",
    "    new_norm = np.sqrt(new_norm)\n",
    "\n",
    "    print(f\"Original norm: {original_norm:.2f}\")\n",
    "    print(f\"Clipped norm: {new_norm:.2f}\")\n",
    "\n",
    "    assert abs(new_norm - 1.0) < 1e-6, f\"Clipped norm should be 1.0, got {new_norm}\"\n",
    "\n",
    "    ## test small gradients that don;t need clipping\n",
    "    small_param = Tensor([1.0, 2.0], requires_grad= True)\n",
    "    small_param.grad = np.array([0.1, 0.2])\n",
    "    small_params = [small_param]\n",
    "    original_small = clip_grad_norm(small_params, max_norm= 1.0)\n",
    "    assert original_small < 1.0, \"Small gradients shouldn't be clipped\"\n",
    "\n",
    "    print(\"âœ… Gradient clipping works correctly!\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    test_unit_clip_grad_norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164a438c-55f4-462f-b197-04139546a110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
