{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1434324f-826a-4622-bc6f-6c8c36f7f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Optional\n",
    "\n",
    "from tinytorch.core.tensor import Tensor\n",
    "from tinytorch.core.activations import ReLU\n",
    "from tinytorch.core.layers import Linear\n",
    "from tinytorch.core.losses import log_softmax, MSELoss, CrossEntropyLoss, BinaryCrossEntropyLoss\n",
    "\n",
    "EPSILON = 1e-7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad00367-24b9-42c2-a6be-beca2bacf2d3",
   "metadata": {},
   "source": [
    "## Unit Test: Log_Sum_Exp Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9021a9a-2fb5-446c-8dd2-a621a7f01b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Unit Test: Log-Softmax...\n",
      "‚úÖ log_softmax works correctly with numerical stability!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_log_softmax():\n",
    "    print(\"üî¨ Unit Test: Log-Softmax...\")\n",
    "\n",
    "    # Test baisc functionality\n",
    "    x = Tensor([[1.0, 2.0, 3.0], [0.1, 0.2, 0.9]])\n",
    "    result = log_softmax(x, dim=-1)\n",
    "\n",
    "    # verify lof-sofamx shape preservation \n",
    "    assert result.shape == x.shape, f'shape mismatch: expected {x.shape}, got {result.shape}'\n",
    "\n",
    "    # verify log-softmax properties\n",
    "    softmax_result = np.exp(result.data)\n",
    "    row_sums = np.sum(softmax_result, axis= -1)\n",
    "    assert np.allclose(row_sums, 1.0, atol= 1e-6), f\"Softmax doesn't sum to 1: {row_sums}\"\n",
    "\n",
    "    # Test numerical stability with large values\n",
    "    large_x = Tensor([[100.0, 101.0, 102.0]])\n",
    "    large_result = log_softmax(large_x, dim=-1)\n",
    "    assert not np.any(np.isnan(large_result.data)), f'NaN values in result with large inputs'\n",
    "    assert not np.any(np.isinf(large_result.data)), f'Inf values in result with large input'\n",
    "\n",
    "    print(\"‚úÖ log_softmax works correctly with numerical stability!\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    test_unit_log_softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d16b88d-92c9-4452-bb5d-d34ce18e58be",
   "metadata": {},
   "source": [
    "## Unit Test: MSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdab40f0-481c-46b1-ab36-85e43970e9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Unit Test: MSE Loss...\n",
      "‚úÖ MSELoss works correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_mse_loss():\n",
    "    print(\"üî¨ Unit Test: MSE Loss...\")\n",
    "    loss_fn = MSELoss()\n",
    "\n",
    "    # Test perfec predictions (loss should be 0)\n",
    "    predictions= Tensor([1.0, 2.0, 3.0])\n",
    "    targets = Tensor([1.0, 2.0, 3.0])\n",
    "    perfect_loss = loss_fn.forward(predictions, targets)\n",
    "    assert np.allclose(perfect_loss.data, 0.0, atol= EPSILON), f'Perfect predictions should have 0 loss, got {perfect_loss.data}'\n",
    "\n",
    "    # Test known cases\n",
    "    predictions = Tensor([1.0, 2.0, 3.0])\n",
    "    targets = Tensor([1.5, 2.5, 2.8])\n",
    "    loss = loss_fn.forward(predictions, targets)\n",
    "\n",
    "    # Manual calculations\n",
    "    expected_loss = (0.25 + 0.25 + 0.04) / 3\n",
    "    assert np.allclose(loss.data, expected_loss, atol=1e-6), f'Expected {expected_loss}, got {loss.data}'\n",
    "\n",
    "    # Test that loss is always non-negative\n",
    "    random_pred = Tensor(np.random.randn(10))\n",
    "    random_target = Tensor(np.random.randn(10))\n",
    "    random_loss = loss_fn.forward(random_pred, random_target)\n",
    "    assert random_loss.data >= 0, f'MSR loss should be non-negative, got {random_loss.data}'\n",
    "    \n",
    "\n",
    "    print(\"‚úÖ MSELoss works correctly!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_unit_mse_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d59a47-6031-468c-8221-a4000c7910da",
   "metadata": {},
   "source": [
    "## Unit Test - Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78f20840-f4f3-4f46-b23c-61d7dfe81656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Unit Test: Cross-Entropy Loss...\n",
      "‚úÖ CrossEntropyLoss works correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_cross_entropy_loss():\n",
    "    print(\"üî¨ Unit Test: Cross-Entropy Loss...\")\n",
    "\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "\n",
    "    # Test for perfect predictions\n",
    "    perfect_logits = Tensor([[10.0, -10.0, -10.0],[-10.0, 10.0, -10.0]])\n",
    "    targets = Tensor([0, 1])\n",
    "    perfect_loss = loss_fn.forward(perfect_logits, targets)\n",
    "    assert perfect_loss.data < 0.01, f'Perfect predictions should have very low loss, got {perfect_loss.data}'\n",
    "\n",
    "    # Test uniform predictions (should have loss approximate log(num_classes))\n",
    "    uniform_logits = Tensor([[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]) \n",
    "    uniform_targets = Tensor([0, 1])\n",
    "    uniform_loss = loss_fn.forward(uniform_logits, uniform_targets)\n",
    "    expected_uniform_loss = np.log(3)\n",
    "    assert np.allclose(uniform_loss.data, expected_uniform_loss, atol=0.1), f\"Uniform predictions should have loss ‚âà log(3) = {expected_uniform_loss:.3f}, got {uniform_loss.data:.3f}\"\n",
    "\n",
    "    # Test that wronf confident predictions have high loss\n",
    "    wrong_logits = Tensor([[10.0, -10.0, -10.0], [-10.0, -10.0, 10.0]])\n",
    "    wrong_targets = Tensor([1, 1])\n",
    "    wrong_loss = loss_fn.forward(wrong_logits, wrong_targets)\n",
    "    assert wrong_loss.data > 5.0, f\"Wrong confident predictions should have high loss, got {wrong_loss.data}\"\n",
    "\n",
    "    # Test numerical stability with large logits\n",
    "    large_logits = Tensor([[100.0, 50.0, 25.0]])\n",
    "    large_targets = Tensor([0])\n",
    "    large_loss = loss_fn.forward(large_logits, large_targets)\n",
    "    assert not np.isnan(large_loss.data), \"Loss should not be NaN with large logits\"\n",
    "    assert not np.isinf(large_loss.data)\n",
    "    \n",
    "    print(\"‚úÖ CrossEntropyLoss works correctly!\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    test_unit_cross_entropy_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33329936-d1aa-4fc3-aafe-fb2833350afa",
   "metadata": {},
   "source": [
    "## Unit Test - Binary Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1639cd72-1863-41e6-89b8-7acad58b9556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Unit Test: Binary Cross-Entropy Loss...\n",
      "‚úÖ BinaryCrossEntropyLoss works correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_binary_cross_entropy_loss():\n",
    "    print(\"üî¨ Unit Test: Binary Cross-Entropy Loss...\")\n",
    "\n",
    "    loss_fn = BinaryCrossEntropyLoss()\n",
    "    # test for perfect predictions\n",
    "    perfect_predictions = Tensor([0.9999, 0.0001, 0.9999, 0.0001])\n",
    "    targets = Tensor([1.0, 0.0, 1.0, 0.0])\n",
    "    perfect_loss = loss_fn.forward(perfect_predictions, targets)\n",
    "    assert perfect_loss.data < 0.01, f'Perfect predictions should have very low loss,  got {perfect_loss.data}'\n",
    "\n",
    "    # Test worst predictions\n",
    "    worst_predictions = Tensor([0.0001, 0.9999, 0.0001, 0.9999])\n",
    "    worst_targets = Tensor([1.0, 0.0, 1.0, 0.0])\n",
    "    worst_loss = loss_fn.forward(worst_predictions, worst_targets)\n",
    "    assert worst_loss.data > 5.0, f\"Worst predictions should have high loss, got {worst_loss.data}\"\n",
    "\n",
    "    # Test uniform predictions\n",
    "    uniform_predictions = Tensor([0.5, 0.5, 0.5, 0.5])\n",
    "    uniform_targets = Tensor([1.0, 0.0, 1.0, 0.0])\n",
    "    uniform_loss = loss_fn.forward(uniform_predictions, uniform_targets)\n",
    "    expected_uniform = -np.log(0.5)\n",
    "    assert np.allclose(uniform_loss.data, expected_uniform, atol=0.01), f\"Uniform predictions should have loss ‚âà {expected_uniform:.3f}, got {uniform_loss.data:.3f}\"\n",
    "\n",
    "    # test numerical stabiliy at biundaries\n",
    "    boundary_predictions = Tensor([0.0, 1.0, 0.0, 1.0])\n",
    "    boundary_targets = Tensor([0.0, 1.0, 1.0, 1.0])\n",
    "    boundary_loss = loss_fn.forward(boundary_predictions, boundary_targets)\n",
    "    assert not np.isnan(boundary_loss.data), 'Loss should not be NaN at boundaries'\n",
    "    assert not np.isinf(boundary_loss.data), \"Loss should not be infinite at boundaries\"\n",
    "    \n",
    "    print(\"‚úÖ BinaryCrossEntropyLoss works correctly!\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    test_unit_binary_cross_entropy_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bb055e-81a3-4ae7-bc1b-a8e3a342fc23",
   "metadata": {},
   "source": [
    "## Integration Testing of Loss Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "646101da-1fbd-4292-9c85-6e2fb47a47c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Analysis: Loss Function Behavior Comparison...\n",
      "\n",
      "1. Regression Scenario (House Price Prediction)\n",
      "   Predictions: [200k, 250k, 300k], Targets: [195k, 260k, 290k]\n",
      "   MSE Loss: 75.00 (thousand¬≤)\n",
      "\n",
      "2. Multi-Class Classification (Image Recognition)\n",
      "   Classes: [cat, dog, bird], Predicted: confident about cat, uncertain about dog\n",
      "   Cross-Entropy Loss: 0.335\n",
      "\n",
      "3. Binary Classification (Spam Detection)\n",
      "   Predictions: [0.9, 0.1, 0.7, 0.3] (spam probabilities)\n",
      "   Binary Cross-Entropy Loss: 0.231\n",
      "\n",
      "üí° Key Insights:\n",
      "   - MSE penalizes large errors heavily (good for continuous values)\n",
      "   - Cross-Entropy encourages confident correct predictions\n",
      "   - Binary Cross-Entropy balances false positives and negatives\n"
     ]
    }
   ],
   "source": [
    "def analyze_loss_behaviors():\n",
    "    \"\"\"\n",
    "    üìä Compare how different loss functions behave with various prediction patterns.\n",
    "\n",
    "    This helps students understand when to use each loss function.\n",
    "    \"\"\"\n",
    "    print(\"üìä Analysis: Loss Function Behavior Comparison...\")\n",
    "\n",
    "    # Initialize loss functions\n",
    "    mse_loss = MSELoss()\n",
    "    ce_loss = CrossEntropyLoss()\n",
    "    bce_loss = BinaryCrossEntropyLoss()\n",
    "\n",
    "    print(\"\\n1. Regression Scenario (House Price Prediction)\")\n",
    "    print(\"   Predictions: [200k, 250k, 300k], Targets: [195k, 260k, 290k]\")\n",
    "    house_pred = Tensor([200.0, 250.0, 300.0])  # In thousands\n",
    "    house_target = Tensor([195.0, 260.0, 290.0])\n",
    "    mse = mse_loss.forward(house_pred, house_target)\n",
    "    print(f\"   MSE Loss: {mse.data:.2f} (thousand¬≤)\")\n",
    "\n",
    "    print(\"\\n2. Multi-Class Classification (Image Recognition)\")\n",
    "    print(\"   Classes: [cat, dog, bird], Predicted: confident about cat, uncertain about dog\")\n",
    "    # Logits: [2.0, 0.5, 0.1] suggests model is most confident about class 0 (cat)\n",
    "    image_logits = Tensor([[2.0, 0.5, 0.1], [0.3, 1.8, 0.2]])  # Two samples\n",
    "    image_targets = Tensor([0, 1])  # First is cat (0), second is dog (1)\n",
    "    ce = ce_loss.forward(image_logits, image_targets)\n",
    "    print(f\"   Cross-Entropy Loss: {ce.data:.3f}\")\n",
    "\n",
    "    print(\"\\n3. Binary Classification (Spam Detection)\")\n",
    "    print(\"   Predictions: [0.9, 0.1, 0.7, 0.3] (spam probabilities)\")\n",
    "    spam_pred = Tensor([0.9, 0.1, 0.7, 0.3])\n",
    "    spam_target = Tensor([1.0, 0.0, 1.0, 0.0])  # 1=spam, 0=not spam\n",
    "    bce = bce_loss.forward(spam_pred, spam_target)\n",
    "    print(f\"   Binary Cross-Entropy Loss: {bce.data:.3f}\")\n",
    "\n",
    "    print(\"\\nüí° Key Insights:\")\n",
    "    print(\"   - MSE penalizes large errors heavily (good for continuous values)\")\n",
    "    print(\"   - Cross-Entropy encourages confident correct predictions\")\n",
    "    print(\"   - Binary Cross-Entropy balances false positives and negatives\")\n",
    "\n",
    "    return mse.data, ce.data, bce.data\n",
    "\n",
    "if __name__=='__main__':\n",
    "    analyze_loss_behaviors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78f15bea-93c3-4bfe-8159-fa3cf7386b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Analysis: Loss Function Behavior Comparison...\n",
      "\n",
      "1. Regression Scenario (House Price Prediction)\n",
      "   Predictions: [200k, 250k, 300k], Targets: [195k, 260k, 290k]\n",
      "   MSE Loss: 75.00 (thousand¬≤)\n",
      "\n",
      "2. Multi-Class Classification (Image Recognition)\n",
      "   Classes: [cat, dog, bird], Predicted: confident about cat, uncertain about dog\n",
      "   Cross-Entropy Loss: 0.335\n",
      "\n",
      "3. Binary Classification (Spam Detection)\n",
      "   Predictions: [0.9, 0.1, 0.7, 0.3] (spam probabilities)\n",
      "   Binary Cross-Entropy Loss: 0.231\n",
      "\n",
      "üí° Key Insights:\n",
      "   - MSE penalizes large errors heavily (good for continuous values)\n",
      "   - Cross-Entropy encourages confident correct predictions\n",
      "   - Binary Cross-Entropy balances false positives and negatives\n",
      "\n",
      "üìä Analysis: Loss Function Sensitivity to Errors...\n",
      "MSE Loss:\n",
      "  Minimum at prediction = 1.02, loss = 0.0003\n",
      "  At prediction = 0.5: loss = 0.0003\n",
      "  At prediction = 0.1: loss = 0.8100\n",
      "\n",
      "Binary Cross-Entropy Loss:\n",
      "  Minimum at prediction = 1.02, loss = 0.0101\n",
      "  At prediction = 0.5: loss = 0.0185\n",
      "  At prediction = 0.1: loss = 2.3026\n",
      "\n",
      "üí° Sensitivity Insights:\n",
      "   - MSE grows quadratically with error distance\n",
      "   - BCE grows logarithmically, heavily penalizing wrong confident predictions\n",
      "   - Both encourage correct predictions but with different curvatures\n"
     ]
    }
   ],
   "source": [
    "def analyze_loss_sensitivity():\n",
    "    \"\"\"\n",
    "    üìä Analyze how sensitive each loss function is to prediction errors.\n",
    "\n",
    "    This demonstrates the different error landscapes created by each loss.\n",
    "    \"\"\"\n",
    "    print(\"\\nüìä Analysis: Loss Function Sensitivity to Errors...\")\n",
    "\n",
    "    # Create a range of prediction errors for analysis\n",
    "    true_value = 1.0\n",
    "    predictions = np.linspace(0.1, 1.9, 50)  # From 0.1 to 1.9\n",
    "\n",
    "    # Initialize loss functions\n",
    "    mse_loss = MSELoss()\n",
    "    bce_loss = BinaryCrossEntropyLoss()\n",
    "\n",
    "    mse_losses = []\n",
    "    bce_losses = []\n",
    "\n",
    "    for pred in predictions:\n",
    "        # MSE analysis\n",
    "        pred_tensor = Tensor([pred])\n",
    "        target_tensor = Tensor([true_value])\n",
    "        mse = mse_loss.forward(pred_tensor, target_tensor)\n",
    "        mse_losses.append(mse.data)\n",
    "\n",
    "        # BCE analysis (clamp prediction to valid probability range)\n",
    "        clamped_pred = max(0.01, min(0.99, pred))\n",
    "        bce_pred_tensor = Tensor([clamped_pred])\n",
    "        bce_target_tensor = Tensor([1.0])  # Target is \"positive class\"\n",
    "        bce = bce_loss.forward(bce_pred_tensor, bce_target_tensor)\n",
    "        bce_losses.append(bce.data)\n",
    "\n",
    "    # Find minimum losses\n",
    "    min_mse_idx = np.argmin(mse_losses)\n",
    "    min_bce_idx = np.argmin(bce_losses)\n",
    "\n",
    "    print(f\"MSE Loss:\")\n",
    "    print(f\"  Minimum at prediction = {predictions[min_mse_idx]:.2f}, loss = {mse_losses[min_mse_idx]:.4f}\")\n",
    "    print(f\"  At prediction = 0.5: loss = {mse_losses[24]:.4f}\")  # Middle of range\n",
    "    print(f\"  At prediction = 0.1: loss = {mse_losses[0]:.4f}\")\n",
    "\n",
    "    print(f\"\\nBinary Cross-Entropy Loss:\")\n",
    "    print(f\"  Minimum at prediction = {predictions[min_bce_idx]:.2f}, loss = {bce_losses[min_bce_idx]:.4f}\")\n",
    "    print(f\"  At prediction = 0.5: loss = {bce_losses[24]:.4f}\")\n",
    "    print(f\"  At prediction = 0.1: loss = {bce_losses[0]:.4f}\")\n",
    "\n",
    "    print(f\"\\nüí° Sensitivity Insights:\")\n",
    "    print(\"   - MSE grows quadratically with error distance\")\n",
    "    print(\"   - BCE grows logarithmically, heavily penalizing wrong confident predictions\")\n",
    "    print(\"   - Both encourage correct predictions but with different curvatures\")\n",
    "\n",
    "# Run integration analysis when developing\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_loss_behaviors()\n",
    "    analyze_loss_sensitivity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a882518-14c7-4953-b7ce-53b5b247e852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Analysis: Numerical Stability in Loss Functions...\n",
      "\n",
      "Log-Softmax Stability Test:\n",
      "Case                 | Max Input | Log-Softmax Min | Numerically Stable?\n",
      "----------------------------------------------------------------------\n",
      "Small logits         |        3 |          -2.408 | ‚úÖ Yes\n",
      "Medium logits        |       30 |         -20.000 | ‚úÖ Yes\n",
      "Large logits         |      300 |        -200.000 | ‚úÖ Yes\n",
      "Very large logits    |      700 |        -200.000 | ‚úÖ Yes\n",
      "\n",
      "üí° Key Insight: Log-sum-exp trick prevents overflow\n",
      "   Without it: exp(700) would cause overflow in standard softmax\n",
      "   With it: We can handle arbitrarily large logits safely\n"
     ]
    }
   ],
   "source": [
    "def analyze_numerical_stability():\n",
    "    \"\"\"\n",
    "    üìä Demonstrate why numerical stability matters in loss computation.\n",
    "\n",
    "    Shows the difference between naive and stable implementations.\n",
    "    \"\"\"\n",
    "    print(\"üìä Analysis: Numerical Stability in Loss Functions...\")\n",
    "\n",
    "    # Test with increasingly large logits\n",
    "    test_cases = [\n",
    "        (\"Small logits\", [1.0, 2.0, 3.0]),\n",
    "        (\"Medium logits\", [10.0, 20.0, 30.0]),\n",
    "        (\"Large logits\", [100.0, 200.0, 300.0]),\n",
    "        (\"Very large logits\", [500.0, 600.0, 700.0])\n",
    "    ]\n",
    "\n",
    "    print(\"\\nLog-Softmax Stability Test:\")\n",
    "    print(\"Case                 | Max Input | Log-Softmax Min | Numerically Stable?\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    for case_name, logits in test_cases:\n",
    "        x = Tensor([logits])\n",
    "\n",
    "        # Our stable implementation\n",
    "        stable_result = log_softmax(x, dim=-1)\n",
    "\n",
    "        max_input = np.max(logits)\n",
    "        min_output = np.min(stable_result.data)\n",
    "        is_stable = not (np.any(np.isnan(stable_result.data)) or np.any(np.isinf(stable_result.data)))\n",
    "\n",
    "        print(f\"{case_name:20} | {max_input:8.0f} | {min_output:15.3f} | {'‚úÖ Yes' if is_stable else '‚ùå No'}\")\n",
    "\n",
    "    print(f\"\\nüí° Key Insight: Log-sum-exp trick prevents overflow\")\n",
    "    print(\"   Without it: exp(700) would cause overflow in standard softmax\")\n",
    "    print(\"   With it: We can handle arbitrarily large logits safely\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    analyze_numerical_stability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "473443af-8ff9-4f7f-b444-590f4d3e0bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Analysis: Numerical Stability in Loss Functions...\n",
      "\n",
      "Log-Softmax Stability Test:\n",
      "Case                 | Max Input | Log-Softmax Min | Numerically Stable?\n",
      "----------------------------------------------------------------------\n",
      "Small logits         |        3 |          -2.408 | ‚úÖ Yes\n",
      "Medium logits        |       30 |         -20.000 | ‚úÖ Yes\n",
      "Large logits         |      300 |        -200.000 | ‚úÖ Yes\n",
      "Very large logits    |      700 |        -200.000 | ‚úÖ Yes\n",
      "\n",
      "üí° Key Insight: Log-sum-exp trick prevents overflow\n",
      "   Without it: exp(700) would cause overflow in standard softmax\n",
      "   With it: We can handle arbitrarily large logits safely\n",
      "\n",
      "üìä Analysis: Loss Function Memory Usage...\n",
      "\n",
      "Memory Usage by Batch Size:\n",
      "Batch Size | MSE (MB) | CrossEntropy (MB) | BCE (MB) | Notes\n",
      "---------------------------------------------------------------------------\n",
      "        32 |     0.00 |          0.26 |    0.00 | Linear scaling\n",
      "       128 |     0.00 |          1.02 |    0.00 | 4√ó first\n",
      "       512 |     0.00 |          4.10 |    0.00 | 16√ó first\n",
      "      1024 |     0.01 |          8.20 |    0.01 | 32√ó first\n",
      "\n",
      "üí° Memory Insights:\n",
      "   - CrossEntropy dominates due to large vocabulary (num_classes)\n",
      "   - Memory scales linearly with batch size\n",
      "   - Intermediate activations (softmax) double CE memory\n",
      "   - For batch=1024, CE needs 8.2MB just for loss computation\n"
     ]
    }
   ],
   "source": [
    "def analyze_loss_memory():\n",
    "    \"\"\"\n",
    "    üìä Analyze memory usage patterns of different loss functions.\n",
    "\n",
    "    Understanding memory helps with batch size decisions.\n",
    "    \"\"\"\n",
    "    print(\"\\nüìä Analysis: Loss Function Memory Usage...\")\n",
    "\n",
    "    batch_sizes = [32, 128, 512, 1024]\n",
    "    num_classes = 1000  # Like ImageNet\n",
    "\n",
    "    print(\"\\nMemory Usage by Batch Size:\")\n",
    "    print(\"Batch Size | MSE (MB) | CrossEntropy (MB) | BCE (MB) | Notes\")\n",
    "    print(\"-\" * 75)\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        # Memory calculations (assuming float32 = 4 bytes)\n",
    "        bytes_per_float = 4\n",
    "\n",
    "        # MSE: predictions + targets (both same size as output)\n",
    "        mse_elements = batch_size * 1  # Regression usually has 1 output\n",
    "        mse_memory = mse_elements * bytes_per_float * 2 / 1e6  # Convert to MB\n",
    "\n",
    "        # CrossEntropy: logits + targets + softmax + log_softmax\n",
    "        ce_logits = batch_size * num_classes\n",
    "        ce_targets = batch_size * 1  # Target indices\n",
    "        ce_softmax = batch_size * num_classes  # Intermediate softmax\n",
    "        ce_total_elements = ce_logits + ce_targets + ce_softmax\n",
    "        ce_memory = ce_total_elements * bytes_per_float / 1e6\n",
    "\n",
    "        # BCE: predictions + targets (binary, so smaller)\n",
    "        bce_elements = batch_size * 1\n",
    "        bce_memory = bce_elements * bytes_per_float * 2 / 1e6\n",
    "\n",
    "        notes = \"Linear scaling\" if batch_size == 32 else f\"{batch_size//32}√ó first\"\n",
    "\n",
    "        print(f\"{batch_size:10} | {mse_memory:8.2f} | {ce_memory:13.2f} | {bce_memory:7.2f} | {notes}\")\n",
    "\n",
    "    print(f\"\\nüí° Memory Insights:\")\n",
    "    print(\"   - CrossEntropy dominates due to large vocabulary (num_classes)\")\n",
    "    print(\"   - Memory scales linearly with batch size\")\n",
    "    print(\"   - Intermediate activations (softmax) double CE memory\")\n",
    "    print(f\"   - For batch=1024, CE needs {ce_memory:.1f}MB just for loss computation\")\n",
    "\n",
    "# Run systems analysis when developing\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_numerical_stability()\n",
    "    analyze_loss_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a02ed89-37ed-4b0e-9bf7-aad1b1d1a39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Production Analysis: Loss Function Engineering Patterns...\n",
      "\n",
      "1. Loss Function Choice by Problem Type:\n",
      "System Type          | Loss Type    | Use Case              | Scale Challenge\n",
      "--------------------------------------------------------------------------------\n",
      "Recommender Systems  | BCE/MSE      | User preference prediction | Billions of interactions\n",
      "Computer Vision      | CrossEntropy | Image classification | 1000+ classes, large batches\n",
      "NLP Translation      | CrossEntropy | Next token prediction | 50k+ vocabulary\n",
      "Medical Diagnosis    | BCE          | Disease probability  | Class imbalance critical\n",
      "Financial Trading    | MSE/Huber    | Price prediction     | Outlier robustness needed\n",
      "\n",
      "2. Engineering Trade-offs:\n",
      "\n",
      "Trade-off                    | Spectrum              | Production Decision\n",
      "-------------------------------------------------------------------------------------\n",
      "CrossEntropy vs Label Smoothing | Stability vs Confidence | Label smoothing prevents overconfident predictions\n",
      "MSE vs Huber Loss            | Sensitivity vs Robustness | Huber is less sensitive to outliers\n",
      "Full Softmax vs Sampled      | Accuracy vs Speed    | Hierarchical softmax for large vocabularies\n",
      "Per-Sample vs Batch Loss     | Accuracy vs Memory   | Batch computation is more memory efficient\n",
      "\n",
      "üí° Production Insights:\n",
      "   - Large vocabularies (50k+ tokens) dominate memory in CrossEntropy\n",
      "   - Batch computation is 10-100√ó more efficient than per-sample\n",
      "   - Numerical stability becomes critical at scale (FP16 training)\n",
      "   - Loss computation is often <5% of total training time\n"
     ]
    }
   ],
   "source": [
    "def analyze_production_patterns():\n",
    "    \"\"\"\n",
    "    üöÄ Analyze loss function patterns in production ML systems.\n",
    "\n",
    "    Real insights from systems perspective.\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Production Analysis: Loss Function Engineering Patterns...\")\n",
    "\n",
    "    print(\"\\n1. Loss Function Choice by Problem Type:\")\n",
    "\n",
    "    scenarios = [\n",
    "        (\"Recommender Systems\", \"BCE/MSE\", \"User preference prediction\", \"Billions of interactions\"),\n",
    "        (\"Computer Vision\", \"CrossEntropy\", \"Image classification\", \"1000+ classes, large batches\"),\n",
    "        (\"NLP Translation\", \"CrossEntropy\", \"Next token prediction\", \"50k+ vocabulary\"),\n",
    "        (\"Medical Diagnosis\", \"BCE\", \"Disease probability\", \"Class imbalance critical\"),\n",
    "        (\"Financial Trading\", \"MSE/Huber\", \"Price prediction\", \"Outlier robustness needed\")\n",
    "    ]\n",
    "\n",
    "    print(\"System Type          | Loss Type    | Use Case              | Scale Challenge\")\n",
    "    print(\"-\" * 80)\n",
    "    for system, loss_type, use_case, challenge in scenarios:\n",
    "        print(f\"{system:20} | {loss_type:12} | {use_case:20} | {challenge}\")\n",
    "\n",
    "    print(\"\\n2. Engineering Trade-offs:\")\n",
    "\n",
    "    trade_offs = [\n",
    "        (\"CrossEntropy vs Label Smoothing\", \"Stability vs Confidence\", \"Label smoothing prevents overconfident predictions\"),\n",
    "        (\"MSE vs Huber Loss\", \"Sensitivity vs Robustness\", \"Huber is less sensitive to outliers\"),\n",
    "        (\"Full Softmax vs Sampled\", \"Accuracy vs Speed\", \"Hierarchical softmax for large vocabularies\"),\n",
    "        (\"Per-Sample vs Batch Loss\", \"Accuracy vs Memory\", \"Batch computation is more memory efficient\")\n",
    "    ]\n",
    "\n",
    "    print(\"\\nTrade-off                    | Spectrum              | Production Decision\")\n",
    "    print(\"-\" * 85)\n",
    "    for trade_off, spectrum, decision in trade_offs:\n",
    "        print(f\"{trade_off:28} | {spectrum:20} | {decision}\")\n",
    "\n",
    "    print(\"\\nüí° Production Insights:\")\n",
    "    print(\"   - Large vocabularies (50k+ tokens) dominate memory in CrossEntropy\")\n",
    "    print(\"   - Batch computation is 10-100√ó more efficient than per-sample\")\n",
    "    print(\"   - Numerical stability becomes critical at scale (FP16 training)\")\n",
    "    print(\"   - Loss computation is often <5% of total training time\")\n",
    "\n",
    "# Run production analysis when developing\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_production_patterns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9547844-b6f3-4b6b-b96f-ef8d0be69f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ RUNNING MODULE INTEGRATION TEST\n",
      "==================================================\n",
      "Running unit tests...\n",
      "üî¨ Unit Test: Log-Softmax...\n",
      "‚úÖ log_softmax works correctly with numerical stability!\n",
      "üî¨ Unit Test: MSE Loss...\n",
      "‚úÖ MSELoss works correctly!\n",
      "üî¨ Unit Test: Cross-Entropy Loss...\n",
      "‚úÖ CrossEntropyLoss works correctly!\n",
      "üî¨ Unit Test: Binary Cross-Entropy Loss...\n",
      "‚úÖ BinaryCrossEntropyLoss works correctly!\n",
      "\n",
      "Running integration scenarios...\n",
      "üî¨ Integration Test: Realistic training scenario...\n",
      "‚úÖ End-to-end loss computation works!\n",
      "‚úÖ All loss functions handle edge cases!\n",
      "‚úÖ Numerical stability verified!\n",
      "\n",
      "==================================================\n",
      "üéâ ALL TESTS PASSED! Module ready for export.\n",
      "Run: tito module complete 04\n"
     ]
    }
   ],
   "source": [
    "def test_module():\n",
    "    \"\"\"üß™ Module Test: Complete Integration\n",
    "\n",
    "    Comprehensive test of entire losses module functionality.\n",
    "\n",
    "    This final test runs before module summary to ensure:\n",
    "    - All unit tests pass\n",
    "    - Functions work together correctly\n",
    "    - Module is ready for integration with TinyTorch\n",
    "    \"\"\"\n",
    "    print(\"üß™ RUNNING MODULE INTEGRATION TEST\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Run all unit tests\n",
    "    print(\"Running unit tests...\")\n",
    "    test_unit_log_softmax()\n",
    "    test_unit_mse_loss()\n",
    "    test_unit_cross_entropy_loss()\n",
    "    test_unit_binary_cross_entropy_loss()\n",
    "\n",
    "    print(\"\\nRunning integration scenarios...\")\n",
    "\n",
    "    # Test realistic end-to-end scenario with previous modules\n",
    "    print(\"üî¨ Integration Test: Realistic training scenario...\")\n",
    "\n",
    "    # Simulate a complete prediction -> loss computation pipeline\n",
    "\n",
    "    # 1. MSE for regression (house price prediction)\n",
    "    house_predictions = Tensor([250.0, 180.0, 320.0, 400.0])  # Predicted prices in thousands\n",
    "    house_actual = Tensor([245.0, 190.0, 310.0, 420.0])       # Actual prices\n",
    "    mse_loss = MSELoss()\n",
    "    house_loss = mse_loss.forward(house_predictions, house_actual)\n",
    "    assert house_loss.data > 0, \"House price loss should be positive\"\n",
    "    assert house_loss.data < 1000, \"House price loss should be reasonable\"\n",
    "\n",
    "    # 2. CrossEntropy for classification (image recognition)\n",
    "    image_logits = Tensor([[2.1, 0.5, 0.3], [0.2, 2.8, 0.1], [0.4, 0.3, 2.2]])  # 3 images, 3 classes\n",
    "    image_labels = Tensor([0, 1, 2])  # Correct class for each image\n",
    "    ce_loss = CrossEntropyLoss()\n",
    "    image_loss = ce_loss.forward(image_logits, image_labels)\n",
    "    assert image_loss.data > 0, \"Image classification loss should be positive\"\n",
    "    assert image_loss.data < 5.0, \"Image classification loss should be reasonable\"\n",
    "\n",
    "    # 3. BCE for binary classification (spam detection)\n",
    "    spam_probabilities = Tensor([0.85, 0.12, 0.78, 0.23, 0.91])\n",
    "    spam_labels = Tensor([1.0, 0.0, 1.0, 0.0, 1.0])  # True spam labels\n",
    "    bce_loss = BinaryCrossEntropyLoss()\n",
    "    spam_loss = bce_loss.forward(spam_probabilities, spam_labels)\n",
    "    assert spam_loss.data > 0, \"Spam detection loss should be positive\"\n",
    "    assert spam_loss.data < 5.0, \"Spam detection loss should be reasonable\"\n",
    "\n",
    "    # 4. Test numerical stability with extreme values\n",
    "    extreme_logits = Tensor([[100.0, -100.0, 0.0]])\n",
    "    extreme_targets = Tensor([0])\n",
    "    extreme_loss = ce_loss.forward(extreme_logits, extreme_targets)\n",
    "    assert not np.isnan(extreme_loss.data), \"Loss should handle extreme values\"\n",
    "    assert not np.isinf(extreme_loss.data), \"Loss should not be infinite\"\n",
    "\n",
    "    print(\"‚úÖ End-to-end loss computation works!\")\n",
    "    print(\"‚úÖ All loss functions handle edge cases!\")\n",
    "    print(\"‚úÖ Numerical stability verified!\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"üéâ ALL TESTS PASSED! Module ready for export.\")\n",
    "    print(\"Run: tito module complete 04\")\n",
    "\n",
    "# Run comprehensive module test\n",
    "if __name__ == \"__main__\":\n",
    "    test_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5144a16d-3f59-4d3f-9adb-1430b26a2c05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
