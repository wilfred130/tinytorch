# TinyTorch

**TinyTorch** is a lightweight, educational deep learning framework inspired by PyTorch.  
It is built from scratch to demonstrate how modern deep learning systems work internally.

TinyTorch focuses on:
- Tensor operations and automatic differentiation
- Core neural network components
- Training pipelines without hidden abstractions
- Mathematical clarity and reproducibility

All modules are accessible via the `tinytorch.core.*` namespace.

---

## Features

- ğŸ”¢ Tensor class with automatic differentiation
- ğŸ”— Computational graph construction
- âš¡ Activation functions
- ğŸ§± Neural network layers
- ğŸ“‰ Loss functions
- ğŸ“¦ Data loading utilities
- ğŸš€ Optimizers
- ğŸ” Training and evaluation loops

---

## Installation

Clone the repository:

```bash
git clone https://github.com/yourusername/tinytorch.git
cd tinytorch
```

## Project Structure

```bash
tinytorch/
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ tensor.py        # Tensor object and autograd engine
â”‚   â”œâ”€â”€ activations.py  # Activation functions
â”‚   â”œâ”€â”€ layers.py       # Neural network layers
â”‚   â”œâ”€â”€ losses.py       # Loss functions
â”‚   â”œâ”€â”€ optimizers.py   # Optimization algorithms
â”‚   â”œâ”€â”€ dataloader.py   # Dataset and DataLoader
â”‚   â”œâ”€â”€ training.py     # Training utilities
â”‚
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ linear_regression.py
â”‚   â”œâ”€â”€ classification.py
â”‚
â”œâ”€â”€ tests/
â”œâ”€â”€ README.md
â””â”€â”€ setup.py

```

## Core modules

```bash
tinytorch.core.tensor
```

The foundation of TinyTorch.

Features:

Multi-dimensional tensors

Reverse-mode automatic differentiation

Gradient tracking

Backpropagation through computational graphs

Example

```bash

from tinytorch.core.tensor import Tensor
from tinytorch.core.autograd import enable_autograd
enable_autograd() # <-- Enables development of computation graph

x = Tensor([1.0, 2.0, 3.0], requires_grad=True)
y = x.sum()
y.backward()

print(x.grad)

```

```bash
tinytorch.core.activations
```

Supported activation functions:

ReLU

Sigmoid

Tanh

Softmax

GELU

```bash
from tinytorch.core.activations import ReLU

relu = ReLU()
output = relu(x)

```

```bash
tinytorch.core.layers
```
Supports

Linear layers

Dropout layers

Layer composition

```bash
from tinytorch.core.layers import Linear

layer = Linear(in_features=3, out_features=2)
output = layer(x)

```

```bash
tinytorch.core.losses
```

Supported loss functions:

Mean Squared Error (MSE)

Cross Entropy Loss

Binary Cross Entropy

```bash
from tinytorch.core.losses import MSELoss

loss_fn = MSELoss()
loss = loss_fn(predictions, targets)

```
```bash
tinytorch.core.optimizers
```
Supported optimizers:

SGD

Adam

AdamW

```bash
from tinytorch.core.optimizers import SGD

optimizer = SGD(model.parameters(), lr=0.01)
optimizer.step()
optimizer.zero_grad()

```
