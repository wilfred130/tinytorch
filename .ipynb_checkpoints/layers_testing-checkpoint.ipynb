{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a366e5de-3481-448f-8384-f98b62cedc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from tinytorch.core.layers import Linear, Dropout, Sequential\n",
    "from tinytorch.core.tensor import Tensor\n",
    "from tinytorch.core.activations import ReLU, Sigmoid\n",
    "from tinytorch.core.autograd import enable_autograd\n",
    "\n",
    "enable_autograd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023121b9-22fa-40e8-b528-4720d14fccc8",
   "metadata": {},
   "source": [
    "## Unit Test: Linear Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35375771-8408-4d4b-ad6a-e497f507fa9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Unit Test: Linear Layer...\n",
      "âœ… Linear layer works correctly!\n"
     ]
    }
   ],
   "source": [
    "XAVIER_SCALE_FACTOR = 2.0\n",
    "\n",
    "def test_unit_linear_layer():\n",
    "    print(\"ðŸ”¬ Unit Test: Linear Layer...\")\n",
    "    layer = Linear(784, 256)\n",
    "\n",
    "    # Test layer creation\n",
    "    assert layer.in_features == 784\n",
    "    assert layer.out_features == 256\n",
    "    assert layer.weight.shape == (784, 256)\n",
    "    assert layer.bias.shape == (256,)\n",
    "\n",
    "    # Test xavier initialization (weights should be reasonably scaled)\n",
    "    weight_std = np.std(layer.weight.data)\n",
    "    expected_std = np.sqrt(XAVIER_SCALE_FACTOR / 784)\n",
    "    assert 0.5 * expected_std < weight_std < 2.0 * expected_std, f'Weight std {weight_std} not close to Xavier {expected_std} '\n",
    "\n",
    "    # Test bias initialization  (should be zeros)\n",
    "    assert np.allclose(layer.bias.data, 0), 'Bias should be initialized to zeros'\n",
    "\n",
    "    # Test forward pass\n",
    "    x = Tensor(np.random.randn(32, 784)) # batch of 32 samples\n",
    "    y = layer.forward(x)\n",
    "    assert y.shape == (32, 256), f'Expected shapr (32, 256, got {y.shape}'\n",
    "    \n",
    "    # Test no bias option\n",
    "    layer_no_bias = Linear(10, 5, bias= False)\n",
    "    assert layer_no_bias.bias is None\n",
    "    params = layer_no_bias.parameters()\n",
    "    assert len(params) == 1 \n",
    "\n",
    "    # Test parameters method\n",
    "    params = layer.parameters()\n",
    "    assert len(params) == 2\n",
    "    assert params[0] is layer.weight\n",
    "    assert params[1] is layer.bias\n",
    "\n",
    "\n",
    "    print(\"âœ… Linear layer works correctly!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_unit_linear_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfa3341-83ba-4226-be9e-bb4fc3a52251",
   "metadata": {},
   "source": [
    "## Edge case test: Linear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e3570c1-4008-4d07-b0ed-d9151af0032e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Edge Case Tests: Linear Layer...\n",
      "âœ… Edge cases handled correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_edge_cases_linear():\n",
    "    print(\"ðŸ”¬ Edge Case Tests: Linear Layer...\")\n",
    "\n",
    "    layer = Linear(10, 5)\n",
    "\n",
    "    # Test single sample (should handle 2D input)\n",
    "    x_2d = Tensor(np.random.randn(1, 10))\n",
    "    y = layer.forward(x_2d)\n",
    "    assert y.shape == (1, 5), 'should handle single cases'\n",
    "\n",
    "    # Test zero batch size (edge case)\n",
    "    x_empty = Tensor(np.random.randn(0, 10))\n",
    "    y_empty = layer.forward(x_empty)\n",
    "    assert y_empty.shape == (0, 5), 'should handle empty batch'\n",
    "\n",
    "    # Test numerical stability with large weights\n",
    "    layer_large = Linear(10, 5)\n",
    "    layer_large.weight.data = np.ones((10, 5)) * 100\n",
    "    x = Tensor(np.ones((1, 10)))\n",
    "    y = layer_large.forward(x)\n",
    "    assert not np.any(np.isnan(y.data)), 'should not produce NaN with large weights'\n",
    "    assert not np.any(np.isinf(y.data)), 'should not produce inf with large weights'\n",
    "\n",
    "    # Test with no bias\n",
    "    layer_no_bias= Linear(10, 5, bias= False)\n",
    "    x = Tensor(np.random.randn(4, 10))\n",
    "    y = layer_no_bias.forward(x)\n",
    "    assert y.shape == (4, 5), 'should work without bias'\n",
    "\n",
    "    print(\"âœ… Edge cases handled correctly!\")\n",
    "if __name__ =='__main__':\n",
    "    test_edge_cases_linear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33c486c-374f-426c-a2db-cf653c2626c5",
   "metadata": {},
   "source": [
    "## Parameter Collections Tests: Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82f46e9b-9b86-43f8-9f5c-5728aaaa2019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Parameter Collection Test: Linear Layer...\n",
      "âœ… Parameter collection works correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_parameter_collection_linear():\n",
    "    print(\"ðŸ”¬ Parameter Collection Test: Linear Layer...\")\n",
    "\n",
    "    layer = Linear(10, 5)\n",
    "\n",
    "    # verify parameter collection works\n",
    "    params = layer.parameters()\n",
    "    assert len(params) == 2, 'should return 2 parameters (weight and bias)'\n",
    "    assert params[0].shape == (10, 5), 'first param should be weight'\n",
    "    assert params[1].shape == (5,), 'second parameter should be bias'\n",
    "\n",
    "    # Test layer without bias\n",
    "    layer_no_bias = Linear(10, 5, bias= False)\n",
    "    params_no_bias = layer_no_bias.parameters()\n",
    "    assert len(params_no_bias) == 1, 'should return 1 parameter (weight only)'\n",
    "    \n",
    "    print(\"âœ… Parameter collection works correctly!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_parameter_collection_linear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1b581e-fefe-45e1-a86d-8cb258fe97d0",
   "metadata": {},
   "source": [
    "## Unit test: Dropout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e50acb0-2083-4bc0-a055-4e444963d9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Unit Test: Dropout Layer...\n",
      "âœ… Dropout layer works correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_dropout_layer():\n",
    "    print(\"ðŸ”¬ Unit Test: Dropout Layer...\")\n",
    "\n",
    "    dropout = Dropout(0.5)\n",
    "    assert dropout.p == 0.5\n",
    "\n",
    "    # test inference mode, should pass unchanged\n",
    "    x = Tensor([1, 2, 3, 4])\n",
    "    y_inference = dropout.forward(x, training= False)\n",
    "    assert np.array_equal(x.data, y_inference.data), 'Inference should pass through unchanged'\n",
    "\n",
    "    # Test training mode with zero dropout (should pass through unchanged)\n",
    "    dropout_zero = Dropout(0.0)\n",
    "    y_zero = dropout_zero.forward(x, training= True)\n",
    "    assert np.array_equal(x.data, y_zero.data), 'Zero dropout should pass through unchanged'\n",
    "\n",
    "    dropout_full = Dropout(1.0)\n",
    "    y_full = dropout_full.forward(x, training= True)\n",
    "    assert np.allclose(y_full.data, 0), 'Full dropout should zero everything'\n",
    "\n",
    "    # Test training mode with partial dropout\n",
    "    np.random.seed(42)\n",
    "    x_large = Tensor(np.ones((1000,)))\n",
    "    y_train = dropout.forward(x_large, training= True)\n",
    "\n",
    "    # count non zero elements\n",
    "    non_zero_count = np.count_nonzero(y_train.data)\n",
    "    expected = 500\n",
    "    std_error = np.sqrt(1000 * 0.5 * 0.5)\n",
    "    lower_bound = expected - 3 * std_error\n",
    "    upper_bound = expected + 3 * std_error\n",
    "    assert lower_bound < non_zero_count < upper_bound, f'Expected {expected}Â±{3*std-error:.0f} survivors, got {non_zero_count}'\n",
    "\n",
    "    # Test sclaing (surviving element should be scaled y 1 / (1- p) = 2.0)\n",
    "    surviving_values = y_train.data[y_train.data != 0]\n",
    "    expected_value = 2.0 \n",
    "    assert np.allclose(surviving_values, expected_value), f'Surviving values should be {expected_value}'\n",
    "\n",
    "    # Test no parameters\n",
    "    params = dropout.parameters()\n",
    "    assert len(params) == 0, 'Dropout should have no parameters'\n",
    "\n",
    "    # Test invalid probabilities\n",
    "    try:\n",
    "        Dropout(-0.1)\n",
    "        assert False, 'Should raise ValueError for negative probability'\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        Dropout(1.1)\n",
    "        assert False, 'Should raise ValueEror for probability > 1'\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    print(\"âœ… Dropout layer works correctly!\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    test_unit_dropout_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfff10a-e572-4b3c-8264-5558e5236cb2",
   "metadata": {},
   "source": [
    "## Integration Bringing it Together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07f4fa25-ee5f-4b1f-ba0a-bfe6cae2ee0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Analyzing Layer Memory Usage...\n",
      "\n",
      "Linear Layer Memory Analysis:\n",
      "Configuration â†’ Weight Memory â†’ Bias Memory â†’ Total Memory\n",
      "( 784,  256) â†’   784.0 KB â†’    1.0 KB â†’   785.0 KB\n",
      "( 256,  256) â†’   256.0 KB â†’    1.0 KB â†’   257.0 KB\n",
      "( 256,   10) â†’    10.0 KB â†’    0.0 KB â†’    10.0 KB\n",
      "(2048, 2048) â†’ 16384.0 KB â†’    8.0 KB â†’ 16392.0 KB\n",
      "\n",
      "ðŸ’¡ Multi-layer Model Memory Scaling:\n",
      "Hidden= 128: 109,386 params =   0.4 MB\n",
      "Hidden= 256: 235,146 params =   0.9 MB\n",
      "Hidden= 512: 535,818 params =   2.0 MB\n",
      "Hidden=1024: 1,333,770 params =   5.1 MB\n",
      "Hidden=2048: 3,716,106 params =  14.2 MB\n"
     ]
    }
   ],
   "source": [
    "def analyze_layer_memory():\n",
    "    \"\"\"ðŸ“Š Analyze memory usage patterns in layer operations.\"\"\"\n",
    "    print(\"ðŸ“Š Analyzing Layer Memory Usage...\")\n",
    "\n",
    "    # Test different layer sizes\n",
    "    layer_configs = [\n",
    "        (784, 256),   # MNIST â†’ hidden\n",
    "        (256, 256),   # Hidden â†’ hidden\n",
    "        (256, 10),    # Hidden â†’ output\n",
    "        (2048, 2048), # Large hidden\n",
    "    ]\n",
    "\n",
    "    print(\"\\nLinear Layer Memory Analysis:\")\n",
    "    print(\"Configuration â†’ Weight Memory â†’ Bias Memory â†’ Total Memory\")\n",
    "\n",
    "    for in_feat, out_feat in layer_configs:\n",
    "        # Calculate memory usage\n",
    "        weight_memory = in_feat * out_feat * 4  # 4 bytes per float32\n",
    "        bias_memory = out_feat * 4\n",
    "        total_memory = weight_memory + bias_memory\n",
    "\n",
    "        print(f\"({in_feat:4d}, {out_feat:4d}) â†’ {weight_memory/1024:7.1f} KB â†’ {bias_memory/1024:6.1f} KB â†’ {total_memory/1024:7.1f} KB\")\n",
    "\n",
    "    # Analyze multi-layer memory scaling\n",
    "    print(\"\\nðŸ’¡ Multi-layer Model Memory Scaling:\")\n",
    "    hidden_sizes = [128, 256, 512, 1024, 2048]\n",
    "\n",
    "    for hidden_size in hidden_sizes:\n",
    "        # 3-layer MLP: 784 â†’ hidden â†’ hidden/2 â†’ 10\n",
    "        layer1_params = 784 * hidden_size + hidden_size\n",
    "        layer2_params = hidden_size * (hidden_size // 2) + (hidden_size // 2)\n",
    "        layer3_params = (hidden_size // 2) * 10 + 10\n",
    "\n",
    "        total_params = layer1_params + layer2_params + layer3_params\n",
    "        memory_mb = total_params * 4 / (1024 * 1024)\n",
    "\n",
    "        print(f\"Hidden={hidden_size:4d}: {total_params:7,} params = {memory_mb:5.1f} MB\")\n",
    "\n",
    "# Analysis will be run in main block\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    analyze_layer_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7c2153b-327f-4af9-aef1-7e6ce6d9251e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Analyzing Layer Computational Complexity...\n",
      "\n",
      "Linear Layer FLOPs Analysis:\n",
      "Batch Size â†’ Matrix Multiply FLOPs â†’ Bias Add FLOPs â†’ Total FLOPs\n",
      "         1 â†’         200,704 â†’           256 â†’     200,960\n",
      "        32 â†’       6,422,528 â†’         8,192 â†’   6,430,720\n",
      "       128 â†’      25,690,112 â†’        32,768 â†’  25,722,880\n",
      "       512 â†’     102,760,448 â†’       131,072 â†’ 102,891,520\n",
      "\n",
      "Linear Layer Timing Analysis:\n",
      "Batch Size â†’ Time (ms) â†’ Throughput (samples/sec)\n",
      "         1 â†’    0.962 ms â†’        1,040 samples/sec\n",
      "        32 â†’   31.517 ms â†’        1,015 samples/sec\n",
      "       128 â†’  101.279 ms â†’        1,264 samples/sec\n",
      "       512 â†’  404.145 ms â†’        1,267 samples/sec\n",
      "\n",
      "ðŸ’¡ Key Insights:\n",
      "ðŸš€ Linear layer complexity: O(batch_size Ã— in_features Ã— out_features)\n",
      "ðŸš€ Memory grows linearly with batch size, quadratically with layer width\n",
      "ðŸš€ Dropout adds minimal computational overhead (element-wise operations)\n",
      "ðŸš€ Larger batches amortize overhead, improving throughput efficiency\n"
     ]
    }
   ],
   "source": [
    "def analyze_layer_performance():\n",
    "    \"\"\"ðŸ“Š Analyze computational complexity of layer operations.\"\"\"\n",
    "    import time\n",
    "\n",
    "    print(\"ðŸ“Š Analyzing Layer Computational Complexity...\")\n",
    "\n",
    "    # Test forward pass FLOPs\n",
    "    batch_sizes = [1, 32, 128, 512]\n",
    "    layer = Linear(784, 256)\n",
    "\n",
    "    print(\"\\nLinear Layer FLOPs Analysis:\")\n",
    "    print(\"Batch Size â†’ Matrix Multiply FLOPs â†’ Bias Add FLOPs â†’ Total FLOPs\")\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        # Matrix multiplication: (batch, in) @ (in, out) = batch * in * out FLOPs\n",
    "        matmul_flops = batch_size * 784 * 256\n",
    "        # Bias addition: batch * out FLOPs\n",
    "        bias_flops = batch_size * 256\n",
    "        total_flops = matmul_flops + bias_flops\n",
    "\n",
    "        print(f\"{batch_size:10d} â†’ {matmul_flops:15,} â†’ {bias_flops:13,} â†’ {total_flops:11,}\")\n",
    "\n",
    "    # Add timing measurements\n",
    "    print(\"\\nLinear Layer Timing Analysis:\")\n",
    "    print(\"Batch Size â†’ Time (ms) â†’ Throughput (samples/sec)\")\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        x = Tensor(np.random.randn(batch_size, 784))\n",
    "\n",
    "        # Warm up\n",
    "        for _ in range(10):\n",
    "            _ = layer.forward(x)\n",
    "\n",
    "        # Time multiple iterations\n",
    "        iterations = 100\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(iterations):\n",
    "            _ = layer.forward(x)\n",
    "        elapsed = time.perf_counter() - start\n",
    "\n",
    "        time_per_forward = (elapsed / iterations) * 1000  # Convert to ms\n",
    "        throughput = (batch_size * iterations) / elapsed\n",
    "\n",
    "        print(f\"{batch_size:10d} â†’ {time_per_forward:8.3f} ms â†’ {throughput:12,.0f} samples/sec\")\n",
    "\n",
    "    print(\"\\nðŸ’¡ Key Insights:\")\n",
    "    print(\"ðŸš€ Linear layer complexity: O(batch_size Ã— in_features Ã— out_features)\")\n",
    "    print(\"ðŸš€ Memory grows linearly with batch size, quadratically with layer width\")\n",
    "    print(\"ðŸš€ Dropout adds minimal computational overhead (element-wise operations)\")\n",
    "    print(\"ðŸš€ Larger batches amortize overhead, improving throughput efficiency\")\n",
    "\n",
    "# Analysis will be run in main block\n",
    "if __name__=='__main__':\n",
    "    analyze_layer_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06c47d32-be32-4da7-9839-2558255660d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    Linear(784, 256),\n",
    "    ReLU(),\n",
    "    Dropout(0.5),\n",
    "    Linear(256, 128),\n",
    "    ReLU(),\n",
    "    Dropout(0.3),\n",
    "    Linear(128, 10), \n",
    "    Sigmoid()\n",
    ")\n",
    "x = Tensor(np.random.random((30, 784)))\n",
    "output = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1699413d-aa8e-44fb-b6f6-884805e3078c",
   "metadata": {},
   "source": [
    "## Module Integration Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b2bf48a-d0b1-44df-83e7-86de5125c81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª RUNNING MODULE INTEGRATION TEST\n",
      "==================================================\n",
      "âœ… Multi-layer network integration works!\n",
      "Running unit tests...\n",
      "ðŸ”¬ Unit Test: Linear Layer...\n",
      "âœ… Linear layer works correctly!\n",
      "ðŸ”¬ Edge Case Tests: Linear Layer...\n",
      "âœ… Edge cases handled correctly!\n",
      "ðŸ”¬ Parameter Collection Test: Linear Layer...\n",
      "âœ… Parameter collection works correctly!\n",
      "ðŸ”¬ Unit Test: Dropout Layer...\n",
      "âœ… Dropout layer works correctly!\n",
      "\n",
      "Running integration scenarios...\n",
      "ðŸ”¬ Integration Test: Multi-layer Network...\n",
      "âœ… Multi-layer network integration works!\n",
      "\n",
      "==================================================\n",
      "ðŸŽ‰ ALL TESTS PASSED! Module ready for export.\n",
      "Run: tito module complete 03_layers\n"
     ]
    }
   ],
   "source": [
    "def test_module():\n",
    "    print(\"ðŸ§ª RUNNING MODULE INTEGRATION TEST\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    print(\"âœ… Multi-layer network integration works!\")\n",
    "    print(\"Running unit tests...\")\n",
    "    test_unit_linear_layer()\n",
    "    test_edge_cases_linear()\n",
    "    test_parameter_collection_linear()\n",
    "    test_unit_dropout_layer()\n",
    "\n",
    "    print(\"\\nRunning integration scenarios...\")\n",
    "\n",
    "    # Test realistic neural network construction with manual composition\n",
    "    print(\"ðŸ”¬ Integration Test: Multi-layer Network...\")\n",
    "\n",
    "    ReLU_class = ReLU\n",
    "    # Build individual layers for manual composition\n",
    "    layer1 = Linear(784, 128)\n",
    "    activation1 = ReLU_class()\n",
    "    dropout1 = Dropout(0.5)\n",
    "    layer2 = Linear(128, 64)\n",
    "    activation2 = ReLU_class()\n",
    "    dropout2 = Dropout(0.3)\n",
    "    layer3 = Linear(64, 10)\n",
    "\n",
    "    # Test end-to-end forward pass with manual composition\n",
    "    batch_size = 16\n",
    "    x = Tensor(np.random.randn(batch_size, 784))\n",
    "\n",
    "    # Manual forward pass\n",
    "    x = layer1.forward(x)\n",
    "    x = activation1.forward(x)\n",
    "    x = dropout1.forward(x)\n",
    "    x = layer2.forward(x)\n",
    "    x = activation2.forward(x)\n",
    "    x = dropout2.forward(x)\n",
    "    output = layer3.forward(x)\n",
    "\n",
    "    assert output.shape == (batch_size, 10), f\"Expected output shape ({batch_size}, 10), got {output.shape}\"\n",
    "\n",
    "    # Test parameter counting from individual layers\n",
    "    all_params = layer1.parameters() + layer2.parameters() + layer3.parameters()\n",
    "    expected_params = 6  # 3 weights + 3 biases from 3 Linear layers\n",
    "    assert len(all_params) == expected_params, f\"Expected {expected_params} parameters, got {len(all_params)}\"\n",
    "\n",
    "    # Test individual layer functionality\n",
    "    test_x = Tensor(np.random.randn(4, 784))\n",
    "    # Test dropout in training vs inference\n",
    "    dropout_test = Dropout(0.5)\n",
    "    train_output = dropout_test.forward(test_x, training=True)\n",
    "    infer_output = dropout_test.forward(test_x, training=False)\n",
    "    assert np.array_equal(test_x.data, infer_output.data), \"Inference mode should pass through unchanged\"\n",
    "\n",
    "    print(\"âœ… Multi-layer network integration works!\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ðŸŽ‰ ALL TESTS PASSED! Module ready for export.\")\n",
    "    print(\"Run: tito module complete 03_layers\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    test_module()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fa80d35-3162-4643-9749-a8b39cb2fdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MODULE 03: LAYERS - COMPREHENSIVE VALIDATION\n",
      "======================================================================\n",
      "ðŸ§ª RUNNING MODULE INTEGRATION TEST\n",
      "==================================================\n",
      "âœ… Multi-layer network integration works!\n",
      "Running unit tests...\n",
      "ðŸ”¬ Unit Test: Linear Layer...\n",
      "âœ… Linear layer works correctly!\n",
      "ðŸ”¬ Edge Case Tests: Linear Layer...\n",
      "âœ… Edge cases handled correctly!\n",
      "ðŸ”¬ Parameter Collection Test: Linear Layer...\n",
      "âœ… Parameter collection works correctly!\n",
      "ðŸ”¬ Unit Test: Dropout Layer...\n",
      "âœ… Dropout layer works correctly!\n",
      "\n",
      "Running integration scenarios...\n",
      "ðŸ”¬ Integration Test: Multi-layer Network...\n",
      "âœ… Multi-layer network integration works!\n",
      "\n",
      "==================================================\n",
      "ðŸŽ‰ ALL TESTS PASSED! Module ready for export.\n",
      "Run: tito module complete 03_layers\n",
      "\n",
      "======================================================================\n",
      "SYSTEMS ANALYSIS\n",
      "======================================================================\n",
      "ðŸ“Š Analyzing Layer Memory Usage...\n",
      "\n",
      "Linear Layer Memory Analysis:\n",
      "Configuration â†’ Weight Memory â†’ Bias Memory â†’ Total Memory\n",
      "( 784,  256) â†’   784.0 KB â†’    1.0 KB â†’   785.0 KB\n",
      "( 256,  256) â†’   256.0 KB â†’    1.0 KB â†’   257.0 KB\n",
      "( 256,   10) â†’    10.0 KB â†’    0.0 KB â†’    10.0 KB\n",
      "(2048, 2048) â†’ 16384.0 KB â†’    8.0 KB â†’ 16392.0 KB\n",
      "\n",
      "ðŸ’¡ Multi-layer Model Memory Scaling:\n",
      "Hidden= 128: 109,386 params =   0.4 MB\n",
      "Hidden= 256: 235,146 params =   0.9 MB\n",
      "Hidden= 512: 535,818 params =   2.0 MB\n",
      "Hidden=1024: 1,333,770 params =   5.1 MB\n",
      "Hidden=2048: 3,716,106 params =  14.2 MB\n",
      "\n",
      "\n",
      "ðŸ“Š Analyzing Layer Computational Complexity...\n",
      "\n",
      "Linear Layer FLOPs Analysis:\n",
      "Batch Size â†’ Matrix Multiply FLOPs â†’ Bias Add FLOPs â†’ Total FLOPs\n",
      "         1 â†’         200,704 â†’           256 â†’     200,960\n",
      "        32 â†’       6,422,528 â†’         8,192 â†’   6,430,720\n",
      "       128 â†’      25,690,112 â†’        32,768 â†’  25,722,880\n",
      "       512 â†’     102,760,448 â†’       131,072 â†’ 102,891,520\n",
      "\n",
      "Linear Layer Timing Analysis:\n",
      "Batch Size â†’ Time (ms) â†’ Throughput (samples/sec)\n",
      "         1 â†’    0.795 ms â†’        1,258 samples/sec\n",
      "        32 â†’   26.453 ms â†’        1,210 samples/sec\n",
      "       128 â†’  116.472 ms â†’        1,099 samples/sec\n",
      "       512 â†’  414.204 ms â†’        1,236 samples/sec\n",
      "\n",
      "ðŸ’¡ Key Insights:\n",
      "ðŸš€ Linear layer complexity: O(batch_size Ã— in_features Ã— out_features)\n",
      "ðŸš€ Memory grows linearly with batch size, quadratically with layer width\n",
      "ðŸš€ Dropout adds minimal computational overhead (element-wise operations)\n",
      "ðŸš€ Larger batches amortize overhead, improving throughput efficiency\n",
      "\n",
      "======================================================================\n",
      "âœ… MODULE 03 COMPLETE!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 70)\n",
    "    print(\"MODULE 03: LAYERS - COMPREHENSIVE VALIDATION\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Run module integration test\n",
    "    test_module()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"SYSTEMS ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Run analysis functions\n",
    "    analyze_layer_memory()\n",
    "    print(\"\\n\")\n",
    "    analyze_layer_performance()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"âœ… MODULE 03 COMPLETE!\")\n",
    "    print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
