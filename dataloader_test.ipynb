{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "163ca3ae-4b45-4715-bc42-a160ca81573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from tinytorch.core.dataloader import Dataset, TensorDataset, DataLoader, RandomHorizontalFlip, RandomCrop, Compose\n",
    "from tinytorch.core.tensor import Tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7cdc78-006c-4fe6-b6a1-157551da1638",
   "metadata": {},
   "source": [
    "## Unit Test - Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a939f36a-4981-4601-8cdb-10af8c064262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Unit Test: Dataset Abstract Base Class...\n",
      "‚úÖ Dataset is properly abstract\n",
      "‚úÖ Dataset interface works correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_dataset():\n",
    "    print(\"üî¨ Unit Test: Dataset Abstract Base Class...\")\n",
    "\n",
    "    try:\n",
    "        dataset = Dataset()\n",
    "        assert False, 'SHould not be able to instantiate abstract Dataset'\n",
    "    except TypeError:\n",
    "        print(\"‚úÖ Dataset is properly abstract\")\n",
    "    print(\"‚úÖ Dataset interface works correctly!\")\n",
    "\n",
    "    class TestDataset(Dataset):\n",
    "        def __init__(self, size):\n",
    "            self.size = size\n",
    "\n",
    "        def __len__(self) -> int:\n",
    "            return self.size\n",
    "\n",
    "        def __getitem__(self, idx: int):\n",
    "            return f'item_{idx}'\n",
    "\n",
    "    dataset = TestDataset(10)\n",
    "    assert len(dataset) == 10\n",
    "    assert dataset[0] == 'item_0'\n",
    "    assert dataset[9] == 'item_9'\n",
    "\n",
    "if __name__=='__main__':\n",
    "    test_unit_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc01c023-8938-4f8c-901c-5f095881041a",
   "metadata": {},
   "source": [
    "## Unit Test - Tensor Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ddb3bde-462a-474e-83bb-1e11cf7b766a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Unit Test: TensorDataset...\n",
      "‚úÖ TensorDataset works correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_tensordataset():\n",
    "    print(\"üî¨ Unit Test: TensorDataset...\")\n",
    "\n",
    "    features = Tensor([[1, 2], [3, 4], [5, 6]])\n",
    "    labels = Tensor([0, 1, 0])\n",
    "    dataset = TensorDataset(features, labels)\n",
    "\n",
    "    # Test length\n",
    "    assert len(dataset) == 3, f\"Expected length 3, got {len(dataset)}\"\n",
    "\n",
    "    # Test indexing\n",
    "    sample = dataset[0]\n",
    "    assert len(sample) == 2, \"Should return tuple with 2 tensors\"\n",
    "    assert np.array_equal(sample[0].data, [1, 2]), f'Wrong features: {sample[0].data}'\n",
    "\n",
    "    # Test error handling\n",
    "    try:\n",
    "        dataset[10]\n",
    "        assert False, \"Shoudl raise IndexError for out of bounds access\"\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    # Test mismatch between tensor sizes\n",
    "    try:\n",
    "        bad_features = Tensor([[1,2], [3, 4]])\n",
    "        bad_labels = Tensor([0, 1, 0])\n",
    "        TensorDataset(bad_features, bad_labels)\n",
    "        assert False, \"Should riase error for mismatched tensor sizes\"\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    print(\"‚úÖ TensorDataset works correctly!\")\n",
    "\n",
    "if __name__ =='__main__':\n",
    "    test_unit_tensordataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc87a71b-f160-4617-9d48-d1dd660c6c87",
   "metadata": {},
   "source": [
    "## Unit Test: Data Augmentation Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "07f32f26-d45d-42f6-9cd1-65e06c78deb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Unit Test: Data Augmentation...\n",
      "   Testing RandomHorizontalFlip...\n",
      "   Testing Compose\n",
      "   Tesing Tensor compatibility\n",
      "   Testing randomness...\n",
      "‚úÖ Data Augmentation works correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_augmentation():\n",
    "    print(\"üî¨ Unit Test: Data Augmentation...\")\n",
    "\n",
    "    # Test random flip\n",
    "    print(\"   Testing RandomHorizontalFlip...\")\n",
    "    flip = RandomHorizontalFlip(p= 1.0)\n",
    "\n",
    "    img = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "    flipped = flip(img)\n",
    "    expected = np.array([[3, 2, 1], [6, 5, 4]])\n",
    "    assert np.array_equal(flipped, expected), f'Flipped failed: {fippled} vs {expected}'\n",
    "\n",
    "    # Test never flip\n",
    "    no_flip = RandomHorizontalFlip(p=0.0)\n",
    "    unchanged = no_flip(img)\n",
    "    assert np.array_equal(unchanged, img), 'p=0 should never flip'\n",
    "\n",
    "    # Test random shape preservation\n",
    "    crop = RandomCrop(32, padding= 4)\n",
    "\n",
    "    # Test with (C, H, W) format (cifar-1 style)\n",
    "    img_chw = np.random.randn(3, 32, 32)\n",
    "    cropped  = crop(img_chw)\n",
    "    assert cropped.shape == (3, 32, 32), f'CHW crop shape wrong: {cropped.shape}'\n",
    "\n",
    "    # Test with (H, W) format\n",
    "    img_hw = np.random.randn(28, 28)\n",
    "    crop_hw = RandomCrop(28, padding = 4)\n",
    "    cropped_hw = crop_hw(img_hw)\n",
    "    assert cropped_hw.shape == (28, 28), f'HW crop shape wrong: {cropped.shape}'\n",
    "\n",
    "    ## Test compose pipeline\n",
    "    print(\"   Testing Compose\")\n",
    "    transforms = Compose([\n",
    "        RandomHorizontalFlip(p=0.5),\n",
    "        RandomCrop(32, padding=4)\n",
    "    ])\n",
    "\n",
    "    img = np.random.randn(3, 32, 32)\n",
    "    augmented = transforms(img)\n",
    "    assert augmented.shape == (3, 32, 32), f'Compose output shape wrong: {augmented.shape}'\n",
    "\n",
    "    # test if trasnform works with tensor\n",
    "    print(\"   Tesing Tensor compatibility\")\n",
    "    tensor_img = Tensor(np.random.randn(3, 32, 32))\n",
    "\n",
    "    flip_result = RandomHorizontalFlip(p=1.0)(tensor_img)\n",
    "    assert isinstance(flip_result, Tensor), f'Flip should return Tensor given Tensor'\n",
    "\n",
    "    crop_result = RandomCrop(32, padding=4)(tensor_img)\n",
    "    assert isinstance(crop_result, Tensor), 'Crop should return Tensor given Tensor'\n",
    "\n",
    "    # Test 5: Randomness verification \n",
    "    print (\"   Testing randomness...\")\n",
    "\n",
    "    flip_random = RandomHorizontalFlip(p=0.5)\n",
    "    \n",
    "    flips  = 0\n",
    "    no_flips = 0\n",
    "    test_img = np.array([[1, 2]])\n",
    "\n",
    "    for _ in range(100):\n",
    "        result = flip_random(test_img)\n",
    "        if np.array_equal(result, np.array([[2, 1]])):\n",
    "            flips += 1\n",
    "        else:\n",
    "            no_flips += 1\n",
    "    assert flips > 20 and no_flips > 0, f'flip randomness seems broken: {flips} flips, {no_flips} no-flips'\n",
    "    \n",
    "    print(\"‚úÖ Data Augmentation works correctly!\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    test_unit_augmentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfbc811-8cc4-4ede-b50e-411bcaad6ee9",
   "metadata": {},
   "source": [
    "## Unit Test - Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "60b4c79a-6577-4c6b-8199-cffed0b48d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Unit Test: DataLoader...\n",
      "‚úÖ DataLoader works correctly!\n",
      "‚úÖ DataLoader works correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_dataloader():\n",
    "    print(\"üî¨ Unit Test: DataLoader...\")\n",
    "\n",
    "    # create test dataset\n",
    "    features= Tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n",
    "    labels = Tensor([0, 1, 0, 1, 0])\n",
    "    dataset = TensorDataset(features, labels)\n",
    "\n",
    "    # Test basic batching no shuffling\n",
    "    loader = DataLoader(dataset, batch_size= 2, shuffle= False)\n",
    "    assert len(loader) == 3, f'Expected 3 batches, got {len(loader)}'\n",
    "\n",
    "    batches = list(loader)\n",
    "    assert len(batches) == 3, f'Expected 3 batches, got {len(batches)}'\n",
    "\n",
    "    # Test first batch\n",
    "    batch_features, batch_labels = batches[0]\n",
    "    assert batch_features.data.shape == (2, 2), f'Wrong batch feature shape: {batch_features.data.shape}'\n",
    "    assert batch_labels.data.shape == (2,), f'Wrong batch label shape: {batch_labels.data.shape}'\n",
    "\n",
    "    # Test last batch (should have 1 sample)\n",
    "    batch_features, batch_labels = batches[2]\n",
    "    assert batch_features.data.shape == (1, 2), f\"Wrong last batch features shape: {batch_features.data.shape}\"\n",
    "    assert batch_labels.data.shape == (1,), f\"Wrong last batch labels shape: {batch_labels.data.shape}\"\n",
    "\n",
    "    # Test data is preserved\n",
    "    assert np.array_equal(batches[0][0].data[0], [1, 2]), \"Fist sample should be [1, 2]\"\n",
    "    assert batches[0][1].data[0] == 0, \"First label should be 0\"\n",
    "\n",
    "    loader_shuffle = DataLoader(dataset, batch_size=5, shuffle=True)\n",
    "    loader_no_shuffle = DataLoader(dataset, batch_size=5, shuffle=False)\n",
    "\n",
    "    batch_shuffle = list(loader_shuffle)[0]\n",
    "    batch_no_shuffle = list(loader_no_shuffle)[0]\n",
    "\n",
    "    # Note: This might occasionally fail due to random chance, but very unlikely\n",
    "    # We'll just test that both contain all the original data\n",
    "    shuffle_features = set(tuple(row) for row in batch_shuffle[0].data)\n",
    "    no_shuffle_features = set(tuple(row) for row in batch_no_shuffle[0].data)\n",
    "    expected_features = {(1, 2), (3, 4), (5, 6), (7, 8), (9, 10)}\n",
    "\n",
    "    assert shuffle_features == expected_features, \"Shuffle should preserve all data\"\n",
    "    assert no_shuffle_features == expected_features, \"No shuffle should preserve all data\"\n",
    "\n",
    "    print(\"‚úÖ DataLoader works correctly!\")\n",
    "\n",
    "    print(\"‚úÖ DataLoader works correctly!\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    test_unit_dataloader() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ee09dd4b-4371-44b1-8a7f-75ae01d2d124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Unit Test: DataLoader Deterministic Shuffling...\n",
      "‚úÖ Deterministic shuffling works correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_dataloader_deterministic():\n",
    "    \"\"\"üî¨ Test DataLoader deterministic shuffling with fixed seed.\"\"\"\n",
    "    print(\"üî¨ Unit Test: DataLoader Deterministic Shuffling...\")\n",
    "\n",
    "    # Create test dataset\n",
    "    features = Tensor([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "    labels = Tensor([0, 1, 0, 1])\n",
    "    dataset = TensorDataset(features, labels)\n",
    "\n",
    "    # Test that same seed produces same shuffle\n",
    "    random.seed(42)\n",
    "    loader1 = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "    batches1 = list(loader1)\n",
    "\n",
    "    random.seed(42)\n",
    "    loader2 = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "    batches2 = list(loader2)\n",
    "\n",
    "    # Should produce identical batches with same seed\n",
    "    for i, (batch1, batch2) in enumerate(zip(batches1, batches2)):\n",
    "        assert np.array_equal(batch1[0].data, batch2[0].data), \\\n",
    "            f\"Batch {i} features should be identical with same seed\"\n",
    "        assert np.array_equal(batch1[1].data, batch2[1].data), \\\n",
    "            f\"Batch {i} labels should be identical with same seed\"\n",
    "\n",
    "    # Test that different seeds produce different shuffles\n",
    "    random.seed(42)\n",
    "    loader3 = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "    batches3 = list(loader3)\n",
    "\n",
    "    random.seed(123)  # Different seed\n",
    "    loader4 = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "    batches4 = list(loader4)\n",
    "\n",
    "    # Should produce different batches with different seeds (very likely)\n",
    "    different = False\n",
    "    for batch3, batch4 in zip(batches3, batches4):\n",
    "        if not np.array_equal(batch3[0].data, batch4[0].data):\n",
    "            different = True\n",
    "            break\n",
    "\n",
    "    assert different, \"Different seeds should produce different shuffles\"\n",
    "\n",
    "    print(\"‚úÖ Deterministic shuffling works correctly!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_unit_dataloader_deterministic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd9340a-055b-4574-9325-62d49b0e15f4",
   "metadata": {},
   "source": [
    "## Working with Real Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0915f469-307a-4cd2-b731-f1c9ec68760a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Analyzing DataLoader Performance...\n",
      "\n",
      "üîç Batch Size vs Loading Time:\n",
      "\n",
      "Dataset size: 1000 samples\n",
      "  Batch size  16: 0.028s (35,537 samples/sec)\n",
      "  Batch size  64: 0.032s (31,695 samples/sec)\n",
      "  Batch size 256: 0.015s (65,848 samples/sec)\n",
      "\n",
      "Dataset size: 5000 samples\n",
      "  Batch size  16: 0.073s (68,077 samples/sec)\n",
      "  Batch size  64: 0.056s (89,695 samples/sec)\n",
      "  Batch size 256: 0.058s (86,436 samples/sec)\n",
      "\n",
      "Dataset size: 10000 samples\n",
      "  Batch size  16: 0.133s (75,181 samples/sec)\n",
      "  Batch size  64: 0.112s (89,506 samples/sec)\n",
      "  Batch size 256: 0.128s (78,277 samples/sec)\n",
      "\n",
      "üîÑ Shuffle Overhead Analysis:\n",
      "  No shuffle: 0.108s\n",
      "  With shuffle: 0.122s\n",
      "  Shuffle overhead: 13.7%\n",
      "\n",
      "üí° Key Insights:\n",
      "‚Ä¢ Larger batch sizes reduce per-sample overhead\n",
      "‚Ä¢ Shuffle adds minimal overhead for reasonable dataset sizes\n",
      "‚Ä¢ Memory usage scales linearly with batch size\n",
      "üöÄ Production tip: Balance batch size with GPU memory limits\n",
      "\n",
      "üìä Analyzing Memory Usage Patterns...\n",
      "\n",
      "üíæ Memory Usage by Batch Configuration:\n",
      "\n",
      "MNIST (28√ó28):\n",
      "  Batch   1:    0.0 MB\n",
      "  Batch  32:    0.1 MB\n",
      "  Batch 128:    0.4 MB\n",
      "  Batch 512:    1.5 MB\n",
      "\n",
      "CIFAR-10 (32√ó32√ó3):\n",
      "  Batch   1:    0.0 MB\n",
      "  Batch  32:    0.4 MB\n",
      "  Batch 128:    1.5 MB\n",
      "  Batch 512:    6.0 MB\n",
      "\n",
      "ImageNet (224√ó224√ó1):\n",
      "  Batch   1:    0.2 MB\n",
      "  Batch  32:    6.1 MB\n",
      "  Batch 128:   24.5 MB\n",
      "  Batch 512:   98.0 MB\n",
      "\n",
      "üéØ Memory Trade-offs:\n",
      "‚Ä¢ Larger batches: More memory, better GPU utilization\n",
      "‚Ä¢ Smaller batches: Less memory, more noisy gradients\n",
      "‚Ä¢ Sweet spot: Usually 32-128 depending on model size\n",
      "\n",
      "üî¨ Actual Tensor Memory Usage:\n",
      "  Small batch (32√ó784):\n",
      "    - Data only: 98.0 KB\n",
      "    - With object overhead: 98.2 KB\n",
      "  Large batch (512√ó784):\n",
      "    - Data only: 1568.0 KB\n",
      "    - With object overhead: 1568.2 KB\n",
      "  Ratio: 16.0√ó (data scales linearly)\n",
      "\n",
      "üéØ Memory Optimization Tips:\n",
      "‚Ä¢ Object overhead becomes negligible with larger batches\n",
      "‚Ä¢ Use float32 instead of float64 to halve memory usage\n",
      "‚Ä¢ Consider gradient accumulation for effective larger batches\n",
      "\n",
      "üìä Analyzing Collation Overhead...\n",
      "\n",
      "‚ö° Collation Time by Batch Size:\n",
      "  Batch size   8: 0.13ms per batch (125 batches total)\n",
      "  Batch size  32: 0.38ms per batch (32 batches total)\n",
      "  Batch size 128: 1.85ms per batch (8 batches total)\n",
      "  Batch size 512: 8.27ms per batch (2 batches total)\n",
      "\n",
      "üí° Collation Insights:\n",
      "‚Ä¢ Larger batches take longer to collate (more np.stack operations)\n",
      "‚Ä¢ But fewer large batches are more efficient than many small ones\n",
      "‚Ä¢ Optimal: Balance between batch size and iteration overhead\n"
     ]
    }
   ],
   "source": [
    "def analyze_dataloader_performance():\n",
    "    \"\"\"üìä Analyze DataLoader performance characteristics.\"\"\"\n",
    "    print(\"üìä Analyzing DataLoader Performance...\")\n",
    "\n",
    "    # Create test dataset of varying sizes\n",
    "    sizes = [1000, 5000, 10000]\n",
    "    batch_sizes = [16, 64, 256]\n",
    "\n",
    "    print(\"\\nüîç Batch Size vs Loading Time:\")\n",
    "\n",
    "    for size in sizes:\n",
    "        # Create synthetic dataset\n",
    "        features = Tensor(np.random.randn(size, 100))  # 100 features\n",
    "        labels = Tensor(np.random.randint(0, 10, size))\n",
    "        dataset = TensorDataset(features, labels)\n",
    "\n",
    "        print(f\"\\nDataset size: {size} samples\")\n",
    "\n",
    "        for batch_size in batch_sizes:\n",
    "            # Time data loading\n",
    "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            start_time = time.time()\n",
    "            batch_count = 0\n",
    "            for batch in loader:\n",
    "                batch_count += 1\n",
    "            end_time = time.time()\n",
    "\n",
    "            elapsed = end_time - start_time\n",
    "            throughput = size / elapsed if elapsed > 0 else float('inf')\n",
    "\n",
    "            print(f\"  Batch size {batch_size:3d}: {elapsed:.3f}s ({throughput:,.0f} samples/sec)\")\n",
    "\n",
    "    # Analyze shuffle overhead\n",
    "    print(\"\\nüîÑ Shuffle Overhead Analysis:\")\n",
    "\n",
    "    dataset_size = 10000\n",
    "    features = Tensor(np.random.randn(dataset_size, 50))\n",
    "    labels = Tensor(np.random.randint(0, 5, dataset_size))\n",
    "    dataset = TensorDataset(features, labels)\n",
    "\n",
    "    batch_size = 64\n",
    "\n",
    "    # No shuffle\n",
    "    loader_no_shuffle = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    start_time = time.time()\n",
    "    batches_no_shuffle = list(loader_no_shuffle)\n",
    "    time_no_shuffle = time.time() - start_time\n",
    "\n",
    "    # With shuffle\n",
    "    loader_shuffle = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    start_time = time.time()\n",
    "    batches_shuffle = list(loader_shuffle)\n",
    "    time_shuffle = time.time() - start_time\n",
    "\n",
    "    shuffle_overhead = ((time_shuffle - time_no_shuffle) / time_no_shuffle) * 100\n",
    "\n",
    "    print(f\"  No shuffle: {time_no_shuffle:.3f}s\")\n",
    "    print(f\"  With shuffle: {time_shuffle:.3f}s\")\n",
    "    print(f\"  Shuffle overhead: {shuffle_overhead:.1f}%\")\n",
    "\n",
    "    print(\"\\nüí° Key Insights:\")\n",
    "    print(\"‚Ä¢ Larger batch sizes reduce per-sample overhead\")\n",
    "    print(\"‚Ä¢ Shuffle adds minimal overhead for reasonable dataset sizes\")\n",
    "    print(\"‚Ä¢ Memory usage scales linearly with batch size\")\n",
    "    print(\"üöÄ Production tip: Balance batch size with GPU memory limits\")\n",
    "\n",
    "\n",
    "def analyze_memory_usage():\n",
    "    \"\"\"üìä Analyze memory usage patterns in data loading.\"\"\"\n",
    "    print(\"\\nüìä Analyzing Memory Usage Patterns...\")\n",
    "\n",
    "    # Memory usage estimation\n",
    "    def estimate_memory_mb(batch_size, feature_size, dtype_bytes=4):\n",
    "        \"\"\"Estimate memory usage for a batch.\"\"\"\n",
    "        return (batch_size * feature_size * dtype_bytes) / (1024 * 1024)\n",
    "\n",
    "    print(\"\\nüíæ Memory Usage by Batch Configuration:\")\n",
    "\n",
    "    feature_sizes = [784, 3072, 50176]  # MNIST, CIFAR-10, ImageNet-like\n",
    "    feature_names = [\"MNIST (28√ó28)\", \"CIFAR-10 (32√ó32√ó3)\", \"ImageNet (224√ó224√ó1)\"]\n",
    "    batch_sizes = [1, 32, 128, 512]\n",
    "\n",
    "    for feature_size, name in zip(feature_sizes, feature_names):\n",
    "        print(f\"\\n{name}:\")\n",
    "        for batch_size in batch_sizes:\n",
    "            memory_mb = estimate_memory_mb(batch_size, feature_size)\n",
    "            print(f\"  Batch {batch_size:3d}: {memory_mb:6.1f} MB\")\n",
    "\n",
    "    print(\"\\nüéØ Memory Trade-offs:\")\n",
    "    print(\"‚Ä¢ Larger batches: More memory, better GPU utilization\")\n",
    "    print(\"‚Ä¢ Smaller batches: Less memory, more noisy gradients\")\n",
    "    print(\"‚Ä¢ Sweet spot: Usually 32-128 depending on model size\")\n",
    "\n",
    "    # Demonstrate actual memory usage with our tensors\n",
    "    print(\"\\nüî¨ Actual Tensor Memory Usage:\")\n",
    "\n",
    "    # Create different sized tensors\n",
    "    tensor_small = Tensor(np.random.randn(32, 784))    # Small batch\n",
    "    tensor_large = Tensor(np.random.randn(512, 784))   # Large batch\n",
    "\n",
    "    # Measure actual memory (data array + object overhead)\n",
    "    small_bytes = tensor_small.data.nbytes\n",
    "    large_bytes = tensor_large.data.nbytes\n",
    "\n",
    "    # Also measure Python object overhead\n",
    "    small_total = sys.getsizeof(tensor_small.data) + sys.getsizeof(tensor_small)\n",
    "    large_total = sys.getsizeof(tensor_large.data) + sys.getsizeof(tensor_large)\n",
    "\n",
    "    print(f\"  Small batch (32√ó784):\")\n",
    "    print(f\"    - Data only: {small_bytes / 1024:.1f} KB\")\n",
    "    print(f\"    - With object overhead: {small_total / 1024:.1f} KB\")\n",
    "    print(f\"  Large batch (512√ó784):\")\n",
    "    print(f\"    - Data only: {large_bytes / 1024:.1f} KB\")\n",
    "    print(f\"    - With object overhead: {large_total / 1024:.1f} KB\")\n",
    "    print(f\"  Ratio: {large_bytes / small_bytes:.1f}√ó (data scales linearly)\")\n",
    "\n",
    "    print(\"\\nüéØ Memory Optimization Tips:\")\n",
    "    print(\"‚Ä¢ Object overhead becomes negligible with larger batches\")\n",
    "    print(\"‚Ä¢ Use float32 instead of float64 to halve memory usage\")\n",
    "    print(\"‚Ä¢ Consider gradient accumulation for effective larger batches\")\n",
    "\n",
    "\n",
    "def analyze_collation_overhead():\n",
    "    \"\"\"üìä Analyze the cost of collating samples into batches.\"\"\"\n",
    "    print(\"\\nüìä Analyzing Collation Overhead...\")\n",
    "\n",
    "    # Test different batch sizes to see collation cost\n",
    "    dataset_size = 1000\n",
    "    feature_size = 100\n",
    "    features = Tensor(np.random.randn(dataset_size, feature_size))\n",
    "    labels = Tensor(np.random.randint(0, 10, dataset_size))\n",
    "    dataset = TensorDataset(features, labels)\n",
    "\n",
    "    print(\"\\n‚ö° Collation Time by Batch Size:\")\n",
    "\n",
    "    for batch_size in [8, 32, 128, 512]:\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        start_time = time.time()\n",
    "        for batch in loader:\n",
    "            pass  # Just iterate, measuring collation overhead\n",
    "        total_time = time.time() - start_time\n",
    "\n",
    "        batches = len(loader)\n",
    "        time_per_batch = (total_time / batches) * 1000  # Convert to ms\n",
    "\n",
    "        print(f\"  Batch size {batch_size:3d}: {time_per_batch:.2f}ms per batch ({batches} batches total)\")\n",
    "\n",
    "    print(\"\\nüí° Collation Insights:\")\n",
    "    print(\"‚Ä¢ Larger batches take longer to collate (more np.stack operations)\")\n",
    "    print(\"‚Ä¢ But fewer large batches are more efficient than many small ones\")\n",
    "    print(\"‚Ä¢ Optimal: Balance between batch size and iteration overhead\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    analyze_dataloader_performance()\n",
    "    analyze_memory_usage()\n",
    "    analyze_collation_overhead()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "aa35cd6a-e00a-4e7b-b885-ce0a9acb175c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.random.randn(32, 765)\n",
    "targets = np.random.randint(0, 10, 32)\n",
    "\n",
    "train_size = int(0.8 * len(features))\n",
    "x_train, x_test = features[:train_size], features[train_size:]\n",
    "y_train, y_test = targets[:train_size], targets[train_size:]\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size= 8, shuffle= True)\n",
    "test_loader = DataLoader(test_dataset, batch_size= 8, shuffle= False)\n",
    "\n",
    "len(train_loader), len(test_loader)\n",
    "\n",
    "# model should handle variable batch sizes\n",
    "# in production monitor gpu utilization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40118436-864c-4083-95be-ce3fe3c6a39e",
   "metadata": {},
   "source": [
    "## Integration Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a79c33b5-d6ff-41c6-916c-da3992ec73d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Integration Test: Training Workflow...\n",
      "üìä Dataset splits:\n",
      "  Training: 800 samples, 25 batches\n",
      "  Validation: 200 samples, 7 batches\n",
      "\n",
      "üèÉ Simulated Training Loop:\n",
      "  Batch 1: 32 samples\n",
      "  Batch 2: 32 samples\n",
      "  Batch 3: 32 samples\n",
      "  Total: 25 batches, 800 samples processed\n",
      "‚úÖ Training integration works correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_training_integration():\n",
    "    \"\"\"üî¨ Test DataLoader integration with training workflow.\"\"\"\n",
    "    print(\"üî¨ Integration Test: Training Workflow...\")\n",
    "\n",
    "    # Create a realistic dataset\n",
    "    num_samples = 1000\n",
    "    num_features = 20\n",
    "    num_classes = 5\n",
    "\n",
    "    # Synthetic classification data\n",
    "    features = Tensor(np.random.randn(num_samples, num_features))\n",
    "    labels = Tensor(np.random.randint(0, num_classes, num_samples))\n",
    "\n",
    "    dataset = TensorDataset(features, labels)\n",
    "\n",
    "    # Create train/val splits\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "\n",
    "    # Manual split (in production, you'd use proper splitting utilities)\n",
    "    train_indices = list(range(train_size))\n",
    "    val_indices = list(range(train_size, len(dataset)))\n",
    "\n",
    "    # Create subset datasets\n",
    "    train_samples = [dataset[i] for i in train_indices]\n",
    "    val_samples = [dataset[i] for i in val_indices]\n",
    "\n",
    "    # Convert back to tensors for TensorDataset\n",
    "    train_features = Tensor(np.stack([sample[0].data for sample in train_samples]))\n",
    "    train_labels = Tensor(np.stack([sample[1].data for sample in train_samples]))\n",
    "    val_features = Tensor(np.stack([sample[0].data for sample in val_samples]))\n",
    "    val_labels = Tensor(np.stack([sample[1].data for sample in val_samples]))\n",
    "\n",
    "    train_dataset = TensorDataset(train_features, train_labels)\n",
    "    val_dataset = TensorDataset(val_features, val_labels)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(f\"üìä Dataset splits:\")\n",
    "    print(f\"  Training: {len(train_dataset)} samples, {len(train_loader)} batches\")\n",
    "    print(f\"  Validation: {len(val_dataset)} samples, {len(val_loader)} batches\")\n",
    "\n",
    "    # Simulate training loop\n",
    "    print(\"\\nüèÉ Simulated Training Loop:\")\n",
    "\n",
    "    epoch_samples = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    for batch_idx, (batch_features, batch_labels) in enumerate(train_loader):\n",
    "        batch_count += 1\n",
    "        epoch_samples += len(batch_features.data)\n",
    "\n",
    "        # Simulate forward pass (just check shapes)\n",
    "        assert batch_features.data.shape[0] <= batch_size, \"Batch size exceeded\"\n",
    "        assert batch_features.data.shape[1] == num_features, \"Wrong feature count\"\n",
    "        assert len(batch_labels.data) == len(batch_features.data), \"Mismatched batch sizes\"\n",
    "\n",
    "        if batch_idx < 3:  # Show first few batches\n",
    "            print(f\"  Batch {batch_idx + 1}: {batch_features.data.shape[0]} samples\")\n",
    "\n",
    "    print(f\"  Total: {batch_count} batches, {epoch_samples} samples processed\")\n",
    "\n",
    "    # Validate that all samples were seen\n",
    "    assert epoch_samples == len(train_dataset), f\"Expected {len(train_dataset)}, processed {epoch_samples}\"\n",
    "\n",
    "    print(\"‚úÖ Training integration works correctly!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_training_integration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a0b837-37a2-4ebb-9e84-ad0c8b6f0ce9",
   "metadata": {},
   "source": [
    "## Module Integration Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1c53eb5d-3dbb-4add-a727-37f59777de93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ RUNNING MODULE INTEGRATION TEST\n",
      "==================================================\n",
      "Running unit tests...\n",
      "üî¨ Unit Test: Dataset Abstract Base Class...\n",
      "‚úÖ Dataset is properly abstract\n",
      "‚úÖ Dataset interface works correctly!\n",
      "üî¨ Unit Test: TensorDataset...\n",
      "‚úÖ TensorDataset works correctly!\n",
      "üî¨ Unit Test: DataLoader...\n",
      "‚úÖ DataLoader works correctly!\n",
      "‚úÖ DataLoader works correctly!\n",
      "üî¨ Unit Test: DataLoader Deterministic Shuffling...\n",
      "‚úÖ Deterministic shuffling works correctly!\n",
      "üî¨ Unit Test: Data Augmentation...\n",
      "   Testing RandomHorizontalFlip...\n",
      "   Testing Compose\n",
      "   Tesing Tensor compatibility\n",
      "   Testing randomness...\n",
      "‚úÖ Data Augmentation works correctly!\n",
      "\n",
      "Running integration scenarios...\n",
      "üî¨ Integration Test: Training Workflow...\n",
      "üìä Dataset splits:\n",
      "  Training: 800 samples, 25 batches\n",
      "  Validation: 200 samples, 7 batches\n",
      "\n",
      "üèÉ Simulated Training Loop:\n",
      "  Batch 1: 32 samples\n",
      "  Batch 2: 32 samples\n",
      "  Batch 3: 32 samples\n",
      "  Total: 25 batches, 800 samples processed\n",
      "‚úÖ Training integration works correctly!\n",
      "üî¨ Integration Test: Augmentation with DataLoader...\n",
      "‚úÖ Augmentation + DataLoader integration works!\n",
      "\n",
      "==================================================\n",
      "üéâ ALL TESTS PASSED! Module ready for export.\n",
      "Run: tito module complete 08\n"
     ]
    }
   ],
   "source": [
    "def test_module():\n",
    "    \"\"\"üß™ Module Test: Complete Integration\n",
    "\n",
    "    Comprehensive test of entire module functionality.\n",
    "\n",
    "    This final test runs before module summary to ensure:\n",
    "    - All unit tests pass\n",
    "    - Functions work together correctly\n",
    "    - Module is ready for integration with TinyTorch\n",
    "    \"\"\"\n",
    "    print(\"üß™ RUNNING MODULE INTEGRATION TEST\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Run all unit tests\n",
    "    print(\"Running unit tests...\")\n",
    "    test_unit_dataset()\n",
    "    test_unit_tensordataset()\n",
    "    test_unit_dataloader()\n",
    "    test_unit_dataloader_deterministic()\n",
    "    test_unit_augmentation()\n",
    "\n",
    "    print(\"\\nRunning integration scenarios...\")\n",
    "\n",
    "    # Test complete workflow\n",
    "    test_training_integration()\n",
    "\n",
    "    # Test augmentation with DataLoader\n",
    "    print(\"üî¨ Integration Test: Augmentation with DataLoader...\")\n",
    "\n",
    "    # Create dataset with augmentation\n",
    "    train_transforms = Compose([\n",
    "        RandomHorizontalFlip(0.5),\n",
    "        RandomCrop(8, padding=2)  # Small images for test\n",
    "    ])\n",
    "\n",
    "    # Simulate CIFAR-style images (C, H, W)\n",
    "    images = np.random.randn(100, 3, 8, 8)\n",
    "    labels = np.random.randint(0, 10, 100)\n",
    "\n",
    "    # Apply augmentation manually (how you'd use in practice)\n",
    "    augmented_images = np.array([train_transforms(img) for img in images])\n",
    "\n",
    "    dataset = TensorDataset(Tensor(augmented_images), Tensor(labels))\n",
    "    loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    batch_count = 0\n",
    "    for batch_x, batch_y in loader:\n",
    "        assert batch_x.shape[1:] == (3, 8, 8), f\"Augmented batch shape wrong: {batch_x.shape}\"\n",
    "        batch_count += 1\n",
    "\n",
    "    assert batch_count > 0, \"DataLoader should produce batches\"\n",
    "    print(\"‚úÖ Augmentation + DataLoader integration works!\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"üéâ ALL TESTS PASSED! Module ready for export.\")\n",
    "    print(\"Run: tito module complete 08\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    test_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cf9657-b459-488a-bfe6-0fa5b97396e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
