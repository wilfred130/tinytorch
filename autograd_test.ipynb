{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcd3521c-5587-44f8-b09f-6c8966fd9cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "from tinytorch.core.tensor import Tensor\n",
    "from tinytorch.core.autograd import Function, AddBackward, MulBackward, MatmulBackward, enable_autograd\n",
    "\n",
    "enable_autograd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74963ecb-96b0-42c3-91fb-50e242d2789d",
   "metadata": {},
   "source": [
    "## Unit Test: Function Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5bbb4cb-8299-4ce0-abbb-275678797734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Unit Test: Function Classes...\n",
      "âœ… Function classes work correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_function_class():\n",
    "    print(\"ðŸ”¬ Unit Test: Function Classes...\")\n",
    "\n",
    "    a = Tensor([1, 2, 3], requires_grad= True)\n",
    "    b = Tensor([4, 5, 6], requires_grad= True)\n",
    "\n",
    "    # Test AddBackward\n",
    "    add_func = AddBackward(a, b)\n",
    "    grad_output = np.array([1, 1, 1])\n",
    "    grad_a, grad_b = add_func.apply(grad_output)\n",
    "    assert np.allclose(grad_a, grad_output), f'AddBackward grad_a fialed: {grad_a}'\n",
    "    assert np.allclose(grad_b, grad_output), f'AddBackward grad_b failed: {grad_b}'\n",
    "\n",
    "    # Test MulBackward\n",
    "    mul_func = MulBackward(a, b)\n",
    "    grad_a, grad_b = mul_func.apply(grad_output)\n",
    "    assert np.allclose(grad_a, b.data), f\"MulBackward grad_a failed: {grad_a}\"\n",
    "    assert np.allclose(grad_b, a.data), f\"MulBackward grad_b failed: {grad_b}\"\n",
    "\n",
    "    # Test MatmulBackward\n",
    "    a_mat = Tensor([[1, 2], [3, 4]], requires_grad= True)\n",
    "    b_mat = Tensor([[5, 6], [7, 8]], requires_grad= True)\n",
    "    matmul_func = MatmulBackward(a_mat, b_mat)\n",
    "    grad_output = np.ones((2, 2))\n",
    "    grad_a, grad_b = matmul_func.apply(grad_output)\n",
    "    assert grad_a.shape == a_mat.shape, f\"MatmulBackward grad_a shape: {grad_a.shape}\"\n",
    "    assert grad_b.shape == b_mat.shape, f\"MatmulBackward grad_b shape: {grad_b.shape}\"\n",
    "    \n",
    "    \n",
    "    print(\"âœ… Function classes work correctly!\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    test_unit_function_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561792d2-a3cd-4935-9ab2-475d2615d9b7",
   "metadata": {},
   "source": [
    "## Unit Test: Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a20fe4f9-221a-4ecd-b7a9-eae45ccdb64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Unit Test: Tensor Autograd Enhancement...\n",
      "âœ… Tensor autograd enhancement works correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_tensor_autograd():\n",
    "    print(\"ðŸ”¬ Unit Test: Tensor Autograd Enhancement...\")\n",
    "\n",
    "    # Test simple gradient computation \n",
    "    x = Tensor([2.0], requires_grad= True)\n",
    "    y = x * 3\n",
    "    z = y + 1 # z = 3x + 1\n",
    "    j = z - 1\n",
    "\n",
    "    z.backward()\n",
    "    assert np.allclose(x.grad, [3.0]), f'Expected [3.0], got {x.grad}'\n",
    "\n",
    "    # Test matrix multiplication gradients\n",
    "    a = Tensor([[1.0, 2.0]], requires_grad= True) # 1x2\n",
    "    b = Tensor([[3.0], [4.0]], requires_grad= True) # 2x1\n",
    "    c = a.matmul(b)\n",
    "\n",
    "    c.backward()\n",
    "    assert np.allclose(a.grad, [[3.0, 4.0]]), f'Expected [[3.0, 4.0]] ,got {a.grad}'\n",
    "    assert np.allclose(b.grad, [[1.0], [2.0]]), f'Expected [[1.0], [2.0]], got {b.grad}'\n",
    "\n",
    "    # Test computation graph with multiple operations\n",
    "    x = Tensor([1.0, 2.0], requires_grad= True)\n",
    "    y = x * 2\n",
    "    z = y.sum()\n",
    "\n",
    "\n",
    "    \n",
    "    z.backward()\n",
    "    assert np.allclose(x.grad, [2.0, 2.0]), f'Expected [2.0, 2.0], got {x.data}'\n",
    "\n",
    "\n",
    "\n",
    "    print(\"âœ… Tensor autograd enhancement works correctly!\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    test_unit_tensor_autograd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836bba62-a105-4072-92b0-ee762362b747",
   "metadata": {},
   "source": [
    "## Module Testing: Automatic differentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa035c90-3e82-44f3-b557-fc660dd6d76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª RUNNING MODULE INTEGRATION TEST\n",
      "==================================================\n",
      "ðŸ”¬ Unit Test: Function Classes...\n",
      "âœ… Function classes work correctly!\n",
      "Integration Test Multi-layer Neural Network...\n",
      "\n",
      "Running integration scenarios...\n",
      "âœ… Multi layer neural network works\n",
      "ðŸ”¬ Integration Test: Gradient Accumulation...\n",
      "âœ… Gradient accumulation works\n",
      "ðŸ”¬ Integration test complex operations\n",
      "âœ… Complex mathematical operations work!\n",
      "\n",
      "==================================================\n",
      "ðŸŽ‰ ALL TESTS PASSED! Module ready for export.\n",
      "Run: tito module complete 06_autograd\n"
     ]
    }
   ],
   "source": [
    "def test_module():\n",
    "    print(\"ðŸ§ª RUNNING MODULE INTEGRATION TEST\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # running all unit tests\n",
    "    test_unit_function_class()\n",
    "    # test_unit_tensor_autograd()\n",
    "\n",
    "    # Multilayer computation graph\n",
    "    print('Integration Test Multi-layer Neural Network...')\n",
    "    print(\"\\nRunning integration scenarios...\")\n",
    "\n",
    "    # create a 3-layer computation: x -> Linear -> Linear -> Loss\n",
    "    x = Tensor([[1.0, 2.0]], requires_grad= True)\n",
    "    W1 = Tensor([[0.5, 0.3, 0.1], [0.2, 0.4, 0.6]], requires_grad= True)\n",
    "    b1 = Tensor([[0.1, 0.2, 0.2]], requires_grad= True)\n",
    "\n",
    "    # First layer\n",
    "    h1 = x.matmul(W1) + b1\n",
    "    assert h1.shape == (1, 3)\n",
    "    assert h1.requires_grad == True\n",
    "\n",
    "    # Second layer\n",
    "    W2 = Tensor([[0.1], [0.2], [0.3]], requires_grad= True)\n",
    "    h2 = h1.matmul(W2)\n",
    "    assert h2.shape == (1, 1)\n",
    "\n",
    "    # compute simple loss\n",
    "    loss = h2 * h2\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # verify all parameters have gradients\n",
    "    assert x.grad is not None\n",
    "    assert W1.grad is not None\n",
    "    assert b1.grad is not None\n",
    "    assert W2.grad is not None\n",
    "    assert x.grad.shape == x.shape\n",
    "    assert W1.grad.shape == W1.shape\n",
    "    print(\"âœ… Multi layer neural network works\")\n",
    "\n",
    "    # Test gradient accumulation\n",
    "    print(\"ðŸ”¬ Integration Test: Gradient Accumulation...\")\n",
    "    x = Tensor([2.0], requires_grad= True)\n",
    "\n",
    "    # First computation \n",
    "    y1 = x * 3\n",
    "    y1.backward()\n",
    "    first_grad = x.grad.copy()\n",
    "\n",
    "    # second computation (should accumulate)\n",
    "    y2 = x*5\n",
    "    y2.backward()\n",
    "\n",
    "    assert np.allclose(x.grad, first_grad + 5.0), f\"Gradients should accumulate\"\n",
    "    print(\"âœ… Gradient accumulation works\")\n",
    "\n",
    "    print(\"ðŸ”¬ Integration test complex operations\")\n",
    "    \n",
    "\n",
    "    # test complex mathematical computations\n",
    "    a = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad= True)\n",
    "    b = Tensor([[2.0, 1.0], [1.0, 2.0]], requires_grad= True)\n",
    "\n",
    "    # complex computations ((a @ b )+ a) *b\n",
    "    temp1 = a.matmul(b)\n",
    "    temp2  = temp1 + a\n",
    "    result = temp2 * b\n",
    "    final = result.sum()\n",
    "\n",
    "    final.backward()\n",
    "\n",
    "    assert a.grad is not None\n",
    "    assert b.grad is not None\n",
    "    assert a.grad.shape == a.shape\n",
    "    assert b.grad.shape == b.shape\n",
    "    \n",
    "    print(\"âœ… Complex mathematical operations work!\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ðŸŽ‰ ALL TESTS PASSED! Module ready for export.\")\n",
    "    print(\"Run: tito module complete 06_autograd\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    test_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85cef43-19ab-48db-b19a-61eb052b280f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
