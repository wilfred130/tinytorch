{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90010d29-8133-4f24-a2c8-65d73f97223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.core.tensor import Tensor\n",
    "from torch.core.activations import Sigmoid, ReLU, Tanh, GELU, Softmax\n",
    "import numpy as np\n",
    "\n",
    "TOLERANCE = 1e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02f93f6-5bc3-401c-a534-5a6284c41777",
   "metadata": {},
   "source": [
    "## Unit Test Sigmoid Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35e22e4e-68ed-456f-8760-1b81d69144dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Unit Test: Sigmoid...\n",
      "âœ… Sigmoid works correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_sigmoid():\n",
    "    print(\"ðŸ”¬ Unit Test: Sigmoid...\")\n",
    "\n",
    "    sigmoid = Sigmoid()\n",
    "    # Test basic cases\n",
    "    x = Tensor([0.0])\n",
    "    result = sigmoid.forward(x)\n",
    "    assert np.allclose(result.data, [0.5]), f'sigmoid(0) should be 0.5, got {result.data}'\n",
    "\n",
    "    # Test range property i.e all output should be in (0, 1)\n",
    "    x = Tensor([-10, -1, 0, 1, 10])\n",
    "    result = sigmoid.forward(x)\n",
    "    assert np.all(result.data > 0) and np.all(result.data < 1), 'All sigmoid outputs should be in (0,1 )'\n",
    "\n",
    "    # Test specific values\n",
    "    x = Tensor([-1000, 1000]) # Extreme values\n",
    "    result = sigmoid.forward(x)\n",
    "    assert np.allclose(result.data[0], 0, atol=TOLERANCE), \"sigmoid(-âˆž) should approach 0\"\n",
    "    assert np.allclose(result.data[1], 1, atol=TOLERANCE), \"sigmoid(+âˆž) should approach 1\"\n",
    "    \n",
    "    \n",
    "    print(\"âœ… Sigmoid works correctly!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_unit_sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5025d418-946f-4cfd-9779-00bdac26e30f",
   "metadata": {},
   "source": [
    "## Unit Test ReLU Activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7966b6d1-75dd-46f6-9500-0b07e5c36daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Unit Test: ReLU...\n",
      "âœ… ReLU works correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_relu():\n",
    "    print(\"ðŸ”¬ Unit Test: ReLU...\")\n",
    "\n",
    "    relu = ReLU()\n",
    "    # Test mixed positvie/negative values\n",
    "    x = Tensor([-2, -1, 0, 1, 2])\n",
    "    result = relu.forward(x)\n",
    "    expected = [0, 0, 0, 1, 2]\n",
    "    assert np.allclose(result.data, expected), f'ReLU failed, expected {expected}, got {result.data}'\n",
    "\n",
    "    # Test all negative values\n",
    "    x = Tensor([-5, -3, -1])\n",
    "    result = relu.forward(x)\n",
    "    assert np.allclose(result.data, [0, 0, 0]), 'ReLU should zero all negative values'\n",
    "\n",
    "    # Test all positive values\n",
    "    x = Tensor([1, 3, 5])\n",
    "    result = relu.forward(x)\n",
    "    assert np.allclose(result.data, [1, 3, 5]), 'ReLU should preserve all positive values'\n",
    "\n",
    "    # Test sparsity property\n",
    "    x = Tensor([-1, -2, -3, 1])\n",
    "    result = relu.forward(x)\n",
    "    zeros = np.sum(result.data == 0)\n",
    "    assert zeros == 3, f'ReLU should create sparsity got {zeros} out of 4'\n",
    "    \n",
    "    print(\"âœ… ReLU works correctly!\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    test_unit_relu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448a03bf-7d2d-4633-81a7-c80a7fc1a6a1",
   "metadata": {},
   "source": [
    "## Unit Test Tanh Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f713a29-21b9-437c-a92a-f24fd7a2a958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Unit Test: Tanh...\n",
      "âœ… Tanh works correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_tanh():\n",
    "    print(\"ðŸ”¬ Unit Test: Tanh...\")\n",
    "\n",
    "    tanh = Tanh()\n",
    "\n",
    "    # Test zero\n",
    "    x = Tensor([0.0])\n",
    "    result = tanh.forward(x)\n",
    "    assert np.allclose(result.data, [0.0]), f'tanh(0) should be 0, got {result.data}'\n",
    "\n",
    "    # Test range property\n",
    "    x = Tensor([-10, -1, 0, 1, 10])\n",
    "    result = tanh.forward(x)\n",
    "    assert np.all(result.data >= -1) and np.all(result.data <= 1), 'All tanh outputs should be in [-1, 1]'\n",
    "\n",
    "    # Test symmetry i.e. tanh(-x) = -tanh(x)\n",
    "    x = Tensor([2.0])\n",
    "    pos_result = tanh.forward(x)\n",
    "    x_neg = Tensor([-2.0])\n",
    "    neg_result = tanh.forward(x_neg)\n",
    "    assert np.allclose(pos_result.data, -neg_result.data), 'tanh should be symmetric: tanh(-x) = -tanh(x)'\n",
    "\n",
    "    # Test extreme values\n",
    "    x = Tensor([-1000, 1000])\n",
    "    result = tanh.forward(x)\n",
    "    assert np.allclose(result.data[0], -1, atol=TOLERANCE), \"tanh(-âˆž) should approach -1\"\n",
    "    assert np.allclose(result.data[1], 1, atol=TOLERANCE), \"tanh(+âˆž) should approach 1\"\n",
    "    \n",
    "    \n",
    "\n",
    "    print(\"âœ… Tanh works correctly!\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    test_unit_tanh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c001201-c182-48de-a385-3a32fec8693e",
   "metadata": {},
   "source": [
    "## Unit Test GELU Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2660a606-45ff-41ec-8707-09294362f2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Unit Test: GELU...\n",
      "âœ… GELU works correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_gelu():\n",
    "    print(\"ðŸ”¬ Unit Test: GELU...\")\n",
    "\n",
    "    gelu = GELU()\n",
    "\n",
    "    # Test zero (should be approximately 0)\n",
    "    x = Tensor([0.0])\n",
    "    result= gelu.forward(x)\n",
    "    assert np.allclose(result.data, [0.0], atol=TOLERANCE), f'GELU(0) should be â‰ˆ0, got {result.data}'\n",
    "\n",
    "    # Test positive values (should be roughly preserved)\n",
    "    x = Tensor([1.0])\n",
    "    result = gelu.forward(x)\n",
    "    assert result.data[0] > 0.8, f'GELU(1) should be â‰ˆ0.84, got {result.data[0]}' \n",
    "\n",
    "    # Test negative values (should be small but not zero)\n",
    "    x = Tensor([-1.0])\n",
    "    result = gelu.forward(x)\n",
    "    assert result.data[0] < 0 and result.data[0] > -0.2, f'GELU(-1) should be â‰ˆ-0.16, got {result.data[0]}'\n",
    "\n",
    "    # Test smoothnes property (no sharp corners like ReLU)\n",
    "    x = Tensor([-0.001, 0.0, 0.001])\n",
    "    result = gelu.forward(x)\n",
    "    diff1 = abs(result.data[1] - result.data[0])\n",
    "    diff2 = abs(result.data[2] - result.data[1])\n",
    "    assert diff1 < 0.01 and diff2 < 0.01, \"GELU should be smooth around zero\"\n",
    "\n",
    "    print(\"âœ… GELU works correctly!\")\n",
    "\n",
    "if __name__=='__main__':\n",
    "    test_unit_gelu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8619c87-c12a-490f-b21b-101f3ed869eb",
   "metadata": {},
   "source": [
    "## Unit Test Softmax Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9882e42-8adc-4c14-a6f6-0e98ce45c70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Unit Test: Softmax...\n",
      "âœ… Softmax works correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_softmax():\n",
    "    print(\"ðŸ”¬ Unit Test: Softmax...\")\n",
    "\n",
    "    softmax = Softmax()\n",
    "\n",
    "    # Test basic probability properties\n",
    "    x = Tensor([1, 2, 3])\n",
    "    result = softmax.forward(x)\n",
    "    assert np.allclose(np.sum(result.data), 1.0), f'Softmax should sum to 1, got {np.sum(result.data)}'\n",
    "\n",
    "    # All values should be positive\n",
    "    assert np.all(result.data > 0), 'All softmax values should be positive'\n",
    "\n",
    "    # all values should be less than one\n",
    "    assert np.all(result.data < 1), 'All softmax values should be less than 1'\n",
    "\n",
    "    # Largest input should get largest output\n",
    "    max_input_idx = np.argmax(x.data)\n",
    "    max_output_idx = np.argmax(result.data)\n",
    "    assert max_input_idx == max_output_idx, 'Largest input should get largest softmax output'\n",
    "\n",
    "    # Test numerical stability with large numbers\n",
    "    x = Tensor([1000, 1001, 1002]) # would overflow without max subtraction\n",
    "    result = softmax.forward(x)\n",
    "    assert np.allclose(np.sum(result.data), 1.0), 'Softmax should handle large numbers'\n",
    "    assert not np.any(np.isnan(result.data)), 'Softmax should not produce NaN'\n",
    "    assert not np.any(np.isinf(result.data)), 'Softmax should not produce infinity'\n",
    "\n",
    "    # Test with 2D tensor (batch dimensions)\n",
    "    x = Tensor([[1, 2], [3, 4]])\n",
    "    result = softmax.forward(x)\n",
    "    assert result.shape == (2, 2), 'softmax should preserve input shape'\n",
    "\n",
    "    # Each row shoul sum to one\n",
    "    row_sums = np.sum(result.data, axis= -1)\n",
    "    assert np.allclose(row_sums, [1.0, 1.0]), \"Each row should sum to 1\"\n",
    "\n",
    "    print(\"âœ… Softmax works correctly!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_unit_softmax()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7908b0-56e2-4833-a2e4-86c35ebb5750",
   "metadata": {},
   "source": [
    "## Integration testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e4fc3ff-f8d9-4a16-ad86-e566027250b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª RUNNING MODULE INTEGRATION TEST\n",
      "==================================================\n",
      "Running unit tests...\n",
      "ðŸ”¬ Unit Test: Sigmoid...\n",
      "âœ… Sigmoid works correctly!\n",
      "ðŸ”¬ Unit Test: ReLU...\n",
      "âœ… ReLU works correctly!\n",
      "ðŸ”¬ Unit Test: Tanh...\n",
      "âœ… Tanh works correctly!\n",
      "ðŸ”¬ Unit Test: GELU...\n",
      "âœ… GELU works correctly!\n",
      "ðŸ”¬ Unit Test: Softmax...\n",
      "âœ… Softmax works correctly!\n",
      "\n",
      "Running integration scenarios...\n",
      "ðŸ”¬ Integration Test: Tensor property preservation...\n",
      "âœ… All activations preserve tensor properties!\n",
      "ðŸ”¬ Integration Test: Softmax dimension handling...\n",
      "âœ… Softmax handles different dimensions correctly!\n",
      "ðŸ”¬ Integration Test: Activation chaining...\n",
      "âœ… Activation chaining works correctly!\n",
      "\n",
      "==================================================\n",
      "ðŸŽ‰ ALL TESTS PASSED! Module ready for export.\n",
      "Run: tito module complete 02\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test_module():\n",
    "    \"\"\"ðŸ§ª Module Test: Complete Integration\n",
    "\n",
    "    Comprehensive test of entire module functionality.\n",
    "\n",
    "    This final test runs before module summary to ensure:\n",
    "    - All unit tests pass\n",
    "    - Functions work together correctly\n",
    "    - Module is ready for integration with TinyTorch\n",
    "    \"\"\"\n",
    "    print(\"ðŸ§ª RUNNING MODULE INTEGRATION TEST\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Run all unit tests\n",
    "    print(\"Running unit tests...\")\n",
    "    test_unit_sigmoid()\n",
    "    test_unit_relu()\n",
    "    test_unit_tanh()\n",
    "    test_unit_gelu()\n",
    "    test_unit_softmax()\n",
    "\n",
    "    print(\"\\nRunning integration scenarios...\")\n",
    "\n",
    "    # Test 1: All activations preserve tensor properties\n",
    "    print(\"ðŸ”¬ Integration Test: Tensor property preservation...\")\n",
    "    test_data = Tensor([[1, -1], [2, -2]])  # 2D tensor\n",
    "\n",
    "    activations = [Sigmoid(), ReLU(), Tanh(), GELU()]\n",
    "    for activation in activations:\n",
    "        result = activation.forward(test_data)\n",
    "        assert result.shape == test_data.shape, f\"Shape not preserved by {activation.__class__.__name__}\"\n",
    "        assert isinstance(result, Tensor), f\"Output not Tensor from {activation.__class__.__name__}\"\n",
    "\n",
    "    print(\"âœ… All activations preserve tensor properties!\")\n",
    "\n",
    "    # Test 2: Softmax works with different dimensions\n",
    "    print(\"ðŸ”¬ Integration Test: Softmax dimension handling...\")\n",
    "    data_3d = Tensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])  # (2, 2, 3)\n",
    "    softmax = Softmax()\n",
    "\n",
    "    # Test different dimensions\n",
    "    result_last = softmax(data_3d, dim=-1)\n",
    "    assert result_last.shape == (2, 2, 3), \"Softmax should preserve shape\"\n",
    "\n",
    "    # Check that last dimension sums to 1\n",
    "    last_dim_sums = np.sum(result_last.data, axis=-1)\n",
    "    assert np.allclose(last_dim_sums, 1.0), \"Last dimension should sum to 1\"\n",
    "\n",
    "    print(\"âœ… Softmax handles different dimensions correctly!\")\n",
    "\n",
    "    # Test 3: Activation chaining (simulating neural network)\n",
    "    print(\"ðŸ”¬ Integration Test: Activation chaining...\")\n",
    "\n",
    "    # Simulate: Input â†’ Linear â†’ ReLU â†’ Linear â†’ Softmax (like a simple network)\n",
    "    x = Tensor([[-1, 0, 1, 2]])  # Batch of 1, 4 features\n",
    "\n",
    "    # Apply ReLU (hidden layer activation)\n",
    "    relu = ReLU()\n",
    "    hidden = relu.forward(x)\n",
    "\n",
    "    # Apply Softmax (output layer activation)\n",
    "    softmax = Softmax()\n",
    "    output = softmax.forward(hidden)\n",
    "\n",
    "    # Verify the chain\n",
    "    assert hidden.data[0, 0] == 0, \"ReLU should zero negative input\"\n",
    "    assert np.allclose(np.sum(output.data), 1.0), \"Final output should be probability distribution\"\n",
    "\n",
    "    print(\"âœ… Activation chaining works correctly!\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ðŸŽ‰ ALL TESTS PASSED! Module ready for export.\")\n",
    "    print(\"Run: tito module complete 02\")\n",
    "\n",
    "# Run comprehensive module test\n",
    "if __name__ == \"__main__\":\n",
    "    test_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794d81ef-3f94-4c37-8795-ba98f95efe64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
